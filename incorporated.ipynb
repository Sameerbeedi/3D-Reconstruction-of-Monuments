{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\college\\imp-doc\\sem6\\GENAI\\project\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import torch\n",
    "import glob\n",
    "import random\n",
    "\n",
    "from PIL import Image, ImageOps\n",
    "from transformers import pipeline\n",
    "\n",
    "\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "from tqdm.auto import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import (\n",
    "    StableDiffusionPipeline,\n",
    "    StableDiffusionImg2ImgPipeline,\n",
    "    DPMSolverMultistepScheduler,\n",
    "    EulerAncestralDiscreteScheduler\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Configuration ---\n",
    "# Default reference image directory\n",
    "DEFAULT_REFERENCE_IMAGES_DIR = r\"D:\\college\\imp-doc\\sem6\\GENAI\\project\\new\\all_images\"\n",
    "\n",
    "# Model configurations\n",
    "SD_MODEL_ID = \"SG161222/Realistic_Vision_V5.1_noVAE\"  # Known for photorealism\n",
    "FALLBACK_MODEL_ID = \"stabilityai/stable-diffusion-2-1\"  # Original model as fallback\n",
    "\n",
    "# Load required models\n",
    "def load_models(device=\"cuda\", use_img2img=True):\n",
    "    \"\"\"Loads the text processing and image generation models.\"\"\"\n",
    "    print(\"Loading models...\")\n",
    "    # Load text processing model for enhancing architectural descriptions\n",
    "    text_processor = pipeline(\"text-generation\", model=\"gpt2-large\")\n",
    "    \n",
    "    # Image generation models\n",
    "    models = {\"text_processor\": text_processor}\n",
    "    \n",
    "    try:\n",
    "        print(f\"Loading SD model: {SD_MODEL_ID}\")\n",
    "        torch_dtype = torch.float16 if device == \"cuda\" else torch.float32\n",
    "        \n",
    "        # Load base text-to-image model\n",
    "        text_to_image = StableDiffusionPipeline.from_pretrained(\n",
    "            SD_MODEL_ID,\n",
    "            torch_dtype=torch_dtype,\n",
    "            safety_checker=None\n",
    "        )\n",
    "        \n",
    "        # Use DPMSolver for faster, quality sampling\n",
    "        text_to_image.scheduler = DPMSolverMultistepScheduler.from_config(\n",
    "            text_to_image.scheduler.config,\n",
    "            algorithm_type=\"dpmsolver++\",\n",
    "            use_karras_sigmas=True\n",
    "        )\n",
    "        \n",
    "        text_to_image = text_to_image.to(device)\n",
    "        models[\"text_to_image\"] = text_to_image\n",
    "        \n",
    "        # Load image-to-image model if requested\n",
    "        if use_img2img:\n",
    "            print(\"Loading img2img model...\")\n",
    "            img2img = StableDiffusionImg2ImgPipeline(\n",
    "                vae=text_to_image.vae,\n",
    "                text_encoder=text_to_image.text_encoder,\n",
    "                tokenizer=text_to_image.tokenizer,\n",
    "                unet=text_to_image.unet,\n",
    "                scheduler=text_to_image.scheduler,\n",
    "                safety_checker=None,\n",
    "                feature_extractor=None,\n",
    "                requires_safety_checker=False\n",
    "            )\n",
    "            img2img = img2img.to(device)\n",
    "            models[\"img2img\"] = img2img\n",
    "            \n",
    "        print(\"Models loaded successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading primary model: {e}\")\n",
    "        print(f\"Falling back to {FALLBACK_MODEL_ID}\")\n",
    "        \n",
    "        # Fall back to original model\n",
    "        text_to_image = StableDiffusionPipeline.from_pretrained(\n",
    "            FALLBACK_MODEL_ID,\n",
    "            torch_dtype=torch_dtype,\n",
    "            safety_checker=None\n",
    "        )\n",
    "        text_to_image.scheduler = EulerAncestralDiscreteScheduler.from_config(\n",
    "            text_to_image.scheduler.config\n",
    "        )\n",
    "        text_to_image = text_to_image.to(device)\n",
    "        models[\"text_to_image\"] = text_to_image\n",
    "        \n",
    "        if use_img2img:\n",
    "            img2img = StableDiffusionImg2ImgPipeline(\n",
    "                vae=text_to_image.vae,\n",
    "                text_encoder=text_to_image.text_encoder,\n",
    "                tokenizer=text_to_image.tokenizer,\n",
    "                unet=text_to_image.unet,\n",
    "                scheduler=text_to_image.scheduler,\n",
    "                safety_checker=None,\n",
    "                feature_extractor=None,\n",
    "                requires_safety_checker=False\n",
    "            )\n",
    "            img2img = img2img.to(device)\n",
    "            models[\"img2img\"] = img2img\n",
    "    \n",
    "    return models\n",
    "\n",
    "# Utility function to load reference images\n",
    "def load_reference_images(directory=DEFAULT_REFERENCE_IMAGES_DIR, limit=None):\n",
    "    \"\"\"\n",
    "    Load reference images from the specified directory.\n",
    "    \n",
    "    Args:\n",
    "        directory: Path to reference images\n",
    "        limit: Maximum number of images to load (None for all)\n",
    "    \n",
    "    Returns:\n",
    "        List of PIL Image objects\n",
    "    \"\"\"\n",
    "    print(f\"Loading reference images from {directory}...\")\n",
    "    image_paths = []\n",
    "    \n",
    "    # Get all image files with common extensions\n",
    "    for ext in ['*.jpg', '*.jpeg', '*.png']:\n",
    "        image_paths.extend(glob.glob(os.path.join(directory, ext)))\n",
    "    \n",
    "    if not image_paths:\n",
    "        raise ValueError(f\"No images found in {directory}\")\n",
    "    \n",
    "    print(f\"Found {len(image_paths)} images\")\n",
    "    \n",
    "    # Limit the number of images if specified\n",
    "    if limit and len(image_paths) > limit:\n",
    "        image_paths = random.sample(image_paths, limit)\n",
    "        print(f\"Randomly selected {limit} images\")\n",
    "    \n",
    "    # Load images\n",
    "    images = []\n",
    "    for path in tqdm(image_paths, desc=\"Loading images\"):\n",
    "        try:\n",
    "            img = Image.open(path).convert(\"RGB\")\n",
    "            images.append({\"path\": path, \"image\": img})\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {path}: {e}\")\n",
    "    \n",
    "    print(f\"Successfully loaded {len(images)} reference images\")\n",
    "    return images\n",
    "\n",
    "# Preprocess reference image for the model\n",
    "def preprocess_reference_image(image, target_size=(768, 768)):\n",
    "    \"\"\"\n",
    "    Preprocess a reference image for use with img2img.\n",
    "    \n",
    "    Args:\n",
    "        image: PIL Image\n",
    "        target_size: Target size as (width, height)\n",
    "    \n",
    "    Returns:\n",
    "        Preprocessed PIL Image\n",
    "    \"\"\"\n",
    "    # Resize while maintaining aspect ratio\n",
    "    image = ImageOps.contain(image, target_size)\n",
    "    \n",
    "    # Create a blank canvas of the target size\n",
    "    canvas = Image.new('RGB', target_size, (255, 255, 255))\n",
    "    \n",
    "    # Paste the resized image centered on the canvas\n",
    "    offset = ((target_size[0] - image.width) // 2, \n",
    "              (target_size[1] - image.height) // 2)\n",
    "    canvas.paste(image, offset)\n",
    "    \n",
    "    return canvas\n",
    "\n",
    "# Extract architectural details from Q&A pairs\n",
    "def extract_architectural_details(qa_text):\n",
    "    \"\"\"\n",
    "    Parse Q&A text to extract key architectural details\n",
    "    that can be used in image generation prompts\n",
    "    \"\"\"\n",
    "    # Extract answer portion from Q&A text\n",
    "    answer_match = re.search(r'Answer: (.*?)(?=$|\\n\\n)', qa_text, re.DOTALL)\n",
    "    if not answer_match:\n",
    "        return None\n",
    "        \n",
    "    answer_text = answer_match.group(1).strip()\n",
    "    \n",
    "    # Extract key visual elements using simple heuristics\n",
    "    # Look for sentences with visual descriptors\n",
    "    visual_sentences = []\n",
    "    \n",
    "    # Keywords that suggest visual elements - expanded list\n",
    "    visual_keywords = [\n",
    "        'feature', 'design', 'decorated', 'carved', 'ornate', 'structure', \n",
    "        'shape', 'pattern', 'motif', 'pillar', 'column', 'arch', 'dome',\n",
    "        'appearance', 'visible', 'style', 'height', 'proportion', 'material',\n",
    "        'stone', 'granite', 'sandstone', 'marble', 'sculpture', 'statue',\n",
    "        'relief', 'facade', 'exterior', 'interior', 'wall', 'ceiling',\n",
    "        'floor', 'entrance', 'gateway', 'courtyard', 'hall', 'chamber',\n",
    "        'shrine', 'sanctum', 'tower', 'spire', 'roof', 'terrace', 'platform',\n",
    "        'steps', 'stairway', 'balcony', 'window', 'door', 'panel',\n",
    "        'frieze', 'cornice', 'capital', 'base', 'plinth', 'pedestal'\n",
    "    ]\n",
    "    \n",
    "    # Extract sentences that contain visual elements\n",
    "    sentences = re.split(r'(?<=[.!?])\\s+', answer_text)\n",
    "    for sentence in sentences:\n",
    "        if any(keyword in sentence.lower() for keyword in visual_keywords):\n",
    "            visual_sentences.append(sentence)\n",
    "    \n",
    "    # Join the visual elements together\n",
    "    if visual_sentences:\n",
    "        return \" \".join(visual_sentences)\n",
    "    else:\n",
    "        # If no specific visual elements found, use the first 3 sentences\n",
    "        return \" \".join(sentences[:min(3, len(sentences))])\n",
    "\n",
    "# Format architectural details into optimized image generation prompts\n",
    "def format_image_prompt(architectural_details, monument_name=None):\n",
    "    \"\"\"\n",
    "    Create a well-structured prompt for image generation\n",
    "    that emphasizes realism and architectural accuracy\n",
    "    \"\"\"\n",
    "    # Hampi-specific architectural components\n",
    "    hampi_components = [\n",
    "        \"Vijayanagara Empire architecture\",\n",
    "        \"16th century Hindu temple\",\n",
    "        \"ancient granite stonework\",\n",
    "        \"intricate stone carvings\",\n",
    "        \"archaeological UNESCO World Heritage site\",\n",
    "        \"South Indian architecture\",\n",
    "        \"Dravidian architectural style\",\n",
    "        \"monolithic stone sculptures\",\n",
    "        \"historic ruins\",\n",
    "        \"ancient temple complex\"\n",
    "    ]\n",
    "    \n",
    "    # Photography style components for realism\n",
    "    photo_style = [\n",
    "        \"professional photography\",\n",
    "        \"8k resolution\",\n",
    "        \"photorealistic\",\n",
    "        \"golden hour lighting\",\n",
    "        \"clear details\",\n",
    "        \"architectural photography\",\n",
    "        \"detailed texture\",\n",
    "        \"sharp focus\",\n",
    "        \"wide angle lens\",\n",
    "        \"HDR photography\",\n",
    "        \"documentary style\",\n",
    "        \"National Geographic quality\"\n",
    "    ]\n",
    "    \n",
    "    # Start with base prompt parts\n",
    "    prompt_parts = []\n",
    "    \n",
    "    # Add monument name if provided\n",
    "    if monument_name:\n",
    "        prompt_parts.append(f\"The {monument_name} at Hampi, India\")\n",
    "    else:\n",
    "        prompt_parts.append(\"A monument at Hampi, India\")\n",
    "    \n",
    "    # Add architectural details\n",
    "    prompt_parts.append(architectural_details)\n",
    "    \n",
    "    # Add 3-4 random Hampi components\n",
    "    prompt_parts.extend(random.sample(hampi_components, k=min(4, len(hampi_components))))\n",
    "    \n",
    "    # Add 3-4 random photo style components\n",
    "    prompt_parts.extend(random.sample(photo_style, k=min(4, len(photo_style))))\n",
    "    \n",
    "    # Combine into final prompt\n",
    "    final_prompt = \", \".join(prompt_parts)\n",
    "    \n",
    "    # Enhanced negative prompt\n",
    "    negative_prompt = \"cartoon, painting, illustration, 3d render, sketch, drawing, anime, blur, grainy, text, watermark, signature, low quality, deformed, disfigured, distorted architecture, unrealistic, fantasy elements, artificial, digital art style, oversaturated, low resolution\"\n",
    "    \n",
    "    return final_prompt, negative_prompt\n",
    "\n",
    "# Generate images from reference images\n",
    "def generate_from_reference(\n",
    "    models, \n",
    "    prompt, \n",
    "    reference_image, \n",
    "    negative_prompt=None,\n",
    "    strength=0.7,\n",
    "    guidance_scale=7.5,\n",
    "    num_inference_steps=50,\n",
    "    seed=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate an image based on a reference image and prompt.\n",
    "    \n",
    "    Args:\n",
    "        models: Dictionary of loaded models\n",
    "        prompt: Text prompt for generation\n",
    "        reference_image: PIL Image to use as reference\n",
    "        negative_prompt: Negative prompt\n",
    "        strength: Strength parameter for img2img (how much to transform)\n",
    "        guidance_scale: Guidance scale for generation\n",
    "        num_inference_steps: Number of inference steps\n",
    "        seed: Random seed for reproducibility\n",
    "    \n",
    "    Returns:\n",
    "        Generated PIL Image\n",
    "    \"\"\"\n",
    "    img2img = models.get(\"img2img\")\n",
    "    if img2img is None:\n",
    "        raise ValueError(\"Image-to-image model not loaded\")\n",
    "    \n",
    "    # Set seed for reproducibility if provided\n",
    "    if seed is not None:\n",
    "        generator = torch.Generator(device=img2img.device).manual_seed(seed)\n",
    "    else:\n",
    "        generator = None\n",
    "    \n",
    "    # Generate image\n",
    "    result = img2img(\n",
    "        prompt=prompt,\n",
    "        image=reference_image,\n",
    "        negative_prompt=negative_prompt,\n",
    "        strength=strength,  # How much to change the reference (0-1)\n",
    "        guidance_scale=guidance_scale,\n",
    "        num_inference_steps=num_inference_steps,\n",
    "        generator=generator\n",
    "    )\n",
    "    \n",
    "    return result.images[0]\n",
    "\n",
    "# Generate images based on architectural descriptions and reference images\n",
    "def generate_images(prompt, negative_prompt, models, num_images=1, output_dir=\"hampi_images\", \n",
    "                   reference_dir=DEFAULT_REFERENCE_IMAGES_DIR, reference_count=5, monument_name=None):\n",
    "    \"\"\"\n",
    "    Generate realistic images of Hampi architecture based on prompts and reference images\n",
    "    \"\"\"\n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Load reference images\n",
    "    reference_images = load_reference_images(reference_dir, limit=reference_count)\n",
    "    if not reference_images:\n",
    "        print(f\"Warning: No reference images found in {reference_dir}. Falling back to direct generation.\")\n",
    "        # Fallback to direct generation if no reference images are available\n",
    "        # This would use the original code's approach\n",
    "        images = models[\"text_to_image\"](\n",
    "            prompt=prompt,\n",
    "            negative_prompt=negative_prompt,\n",
    "            num_images_per_prompt=num_images,\n",
    "            num_inference_steps=50,\n",
    "            guidance_scale=7.5\n",
    "        ).images\n",
    "        \n",
    "        # Save images\n",
    "        image_paths = []\n",
    "        for i, image in enumerate(images):\n",
    "            short_desc = prompt.split('.')[0][:30].replace(\" \", \"_\")\n",
    "            filename = f\"{short_desc}_{i}.png\"\n",
    "            save_path = os.path.join(output_dir, filename)\n",
    "            image.save(save_path)\n",
    "            image_paths.append(save_path)\n",
    "            print(f\"Saved image to {save_path}\")\n",
    "        \n",
    "        return image_paths\n",
    "    \n",
    "    # Generate images using references\n",
    "    image_paths = []\n",
    "    for i in range(num_images):\n",
    "        # Select a random reference image\n",
    "        ref_image_data = random.choice(reference_images)\n",
    "        ref_image = ref_image_data[\"image\"]\n",
    "        ref_path = ref_image_data[\"path\"]\n",
    "        \n",
    "        print(f\"Using reference image: {os.path.basename(ref_path)}\")\n",
    "        \n",
    "        # Preprocess the reference image\n",
    "        processed_ref = preprocess_reference_image(ref_image)\n",
    "        \n",
    "        # Generate with a unique seed for variety\n",
    "        seed = random.randint(0, 2147483647)\n",
    "        strength = random.uniform(0.6, 0.85)  # Vary strength for diversity\n",
    "        \n",
    "        try:\n",
    "            # Generate the image\n",
    "            generated_img = generate_from_reference(\n",
    "                models=models,\n",
    "                prompt=prompt,\n",
    "                reference_image=processed_ref,\n",
    "                negative_prompt=negative_prompt,\n",
    "                strength=strength,\n",
    "                guidance_scale=7.5,\n",
    "                num_inference_steps=50,\n",
    "                seed=seed\n",
    "            )\n",
    "            \n",
    "            # Create filename based on shortened prompt and monument\n",
    "            mon_text = monument_name or \"hampi\"\n",
    "            filename = f\"{mon_text.replace(' ', '_')}_{i+1}_seed{seed}.png\"\n",
    "            save_path = os.path.join(output_dir, filename)\n",
    "            \n",
    "            # Save the image\n",
    "            generated_img.save(save_path)\n",
    "            image_paths.append(save_path)\n",
    "            print(f\"Saved image to {save_path}\")\n",
    "            \n",
    "            # Save a comparison image\n",
    "            comparison = Image.new('RGB', (processed_ref.width * 2, processed_ref.height))\n",
    "            comparison.paste(processed_ref, (0, 0))\n",
    "            comparison.paste(generated_img, (processed_ref.width, 0))\n",
    "            \n",
    "            comparison_path = os.path.join(output_dir, f\"{mon_text.replace(' ', '_')}_{i+1}_comparison.png\")\n",
    "            comparison.save(comparison_path)\n",
    "            print(f\"Saved comparison to {comparison_path}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error generating image {i+1}: {e}\")\n",
    "    \n",
    "    return image_paths\n",
    "\n",
    "# Extract Q&A pairs from vector store or text file\n",
    "def get_qa_pairs(source_type=\"file\", file_path=\"Hampi_Architecture_QA.txt\", vector_store_path=None, query=None):\n",
    "    \"\"\"\n",
    "    Extract Q&A pairs from either:\n",
    "    1. Text file generated by store.ipynb\n",
    "    2. Vector store created by store.ipynb\n",
    "    \"\"\"\n",
    "    qa_pairs = []\n",
    "    \n",
    "    if source_type == \"file\":\n",
    "        # Parse the text file format\n",
    "        if os.path.exists(file_path):\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                content = f.read()\n",
    "            \n",
    "            # Extract Q&A blocks\n",
    "            blocks = re.split(r'-{40,}', content)\n",
    "            for block in blocks:\n",
    "                if not block.strip():\n",
    "                    continue\n",
    "                \n",
    "                # Extract question and answer\n",
    "                q_match = re.search(r'Q\\d+: (.*?)(?=\\n|$)', block)\n",
    "                a_match = re.search(r'A\\d+: (.*?)(?=\\n|$)', block, re.DOTALL)\n",
    "                \n",
    "                if q_match and a_match:\n",
    "                    qa_pairs.append({\n",
    "                        \"question\": q_match.group(1).strip(),\n",
    "                        \"answer\": a_match.group(1).strip(),\n",
    "                    })\n",
    "        else:\n",
    "            print(f\"File not found: {file_path}\")\n",
    "    \n",
    "    elif source_type == \"vector_store\" and vector_store_path and query:\n",
    "        # Load the vector store\n",
    "        embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "        vector_store = Chroma(persist_directory=vector_store_path, embedding_function=embedding_model)\n",
    "        \n",
    "        # Search for relevant Q&A pairs\n",
    "        results = vector_store.similarity_search(query, k=5)\n",
    "        \n",
    "        for doc in results:\n",
    "            # Extract question and answer from the document content\n",
    "            content = doc.page_content\n",
    "            q_match = re.search(r'Question: (.*?)(?=\\nAnswer:|$)', content)\n",
    "            a_match = re.search(r'Answer: (.*?)(?=$)', content, re.DOTALL)\n",
    "            \n",
    "            if q_match and a_match:\n",
    "                qa_pairs.append({\n",
    "                    \"question\": q_match.group(1).strip(),\n",
    "                    \"answer\": a_match.group(1).strip(),\n",
    "                })\n",
    "    \n",
    "    return qa_pairs\n",
    "\n",
    "# Main pipeline function\n",
    "def qa_to_images_pipeline(source_type=\"file\", file_path=\"Hampi_Architecture_QA.txt\", \n",
    "                         vector_store_path=\"./chroma_architecture_qa_db\", query=None,\n",
    "                         num_images=1, output_dir=\"hampi_images\", monument_filters=None,\n",
    "                         reference_dir=DEFAULT_REFERENCE_IMAGES_DIR, reference_count=5,\n",
    "                         device=\"cuda\"):\n",
    "    \"\"\"\n",
    "    Complete pipeline to convert architectural Q&A data to realistic images\n",
    "    using reference-based image generation\n",
    "    \n",
    "    Args:\n",
    "        source_type: 'file' or 'vector_store'\n",
    "        file_path: Path to the QA text file (if source_type is 'file')\n",
    "        vector_store_path: Path to the vector store (if source_type is 'vector_store')\n",
    "        query: Search query for the vector store (if source_type is 'vector_store')\n",
    "        num_images: Number of images to generate per QA pair\n",
    "        output_dir: Directory to save the generated images\n",
    "        monument_filters: List of specific monuments to include (e.g., ['Vitthala Temple'])\n",
    "        reference_dir: Directory containing reference images\n",
    "        reference_count: Number of reference images to use\n",
    "        device: Device to use for generation\n",
    "    \"\"\"\n",
    "    print(\"Loading models...\")\n",
    "    models = load_models(device=device)\n",
    "    \n",
    "    print(\"Retrieving Q&A pairs...\")\n",
    "    qa_pairs = get_qa_pairs(source_type, file_path, vector_store_path, query)\n",
    "    \n",
    "    if not qa_pairs:\n",
    "        print(\"No Q&A pairs found!\")\n",
    "        return []\n",
    "    \n",
    "    print(f\"Found {len(qa_pairs)} Q&A pairs.\")\n",
    "    \n",
    "    # Filter by monument if specified\n",
    "    if monument_filters:\n",
    "        filtered_pairs = []\n",
    "        for pair in qa_pairs:\n",
    "            if any(monument.lower() in pair[\"question\"].lower() for monument in monument_filters):\n",
    "                filtered_pairs.append(pair)\n",
    "        qa_pairs = filtered_pairs\n",
    "        print(f\"Filtered to {len(qa_pairs)} Q&A pairs related to specified monuments.\")\n",
    "    \n",
    "    generated_image_paths = []\n",
    "    \n",
    "    for i, pair in enumerate(qa_pairs):\n",
    "        print(f\"\\nProcessing Q&A pair {i+1}/{len(qa_pairs)}\")\n",
    "        print(f\"Question: {pair['question']}\")\n",
    "        \n",
    "        # Extract monument name from question if possible\n",
    "        monument_match = re.search(r'(Vitthala|Virupaksha|Krishna|Hazara Rama|Lotus Mahal|Elephant Stables)', pair['question'])\n",
    "        monument_name = monument_match.group(1) if monument_match else None\n",
    "        \n",
    "        # Extract architectural details\n",
    "        full_qa_text = f\"Question: {pair['question']}\\nAnswer: {pair['answer']}\"\n",
    "        architectural_details = extract_architectural_details(full_qa_text)\n",
    "        \n",
    "        if not architectural_details:\n",
    "            print(\"Could not extract architectural details, skipping...\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"Extracted details: {architectural_details[:100]}...\")\n",
    "        \n",
    "        # Format image generation prompt\n",
    "        prompt, negative_prompt = format_image_prompt(architectural_details, monument_name)\n",
    "        print(f\"Generated prompt: {prompt[:100]}...\")\n",
    "        \n",
    "        # Generate images using reference-based approach\n",
    "        image_paths = generate_images(\n",
    "            prompt=prompt, \n",
    "            negative_prompt=negative_prompt, \n",
    "            models=models, \n",
    "            num_images=num_images, \n",
    "            output_dir=output_dir,\n",
    "            reference_dir=reference_dir,\n",
    "            reference_count=reference_count,\n",
    "            monument_name=monument_name\n",
    "        )\n",
    "        \n",
    "        generated_image_paths.extend(image_paths)\n",
    "    \n",
    "    print(f\"\\nGeneration complete. Created {len(generated_image_paths)} images in {output_dir}.\")\n",
    "    return generated_image_paths\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running store_utils.py directly for testing...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import uuid\n",
    "import re\n",
    "import random # Added import for random.choice\n",
    "# Updated imports for LangChain components\n",
    "from langchain_community.vectorstores import Chroma # Changed from langchain.vectorstores\n",
    "from langchain_huggingface import HuggingFaceEmbeddings # Changed from langchain_community.embeddings\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.schema import HumanMessage, SystemMessage\n",
    "import traceback # Add traceback for detailed error printing\n",
    "\n",
    "# --- Configuration ---\n",
    "# Define default paths (can be overridden by arguments if needed)\n",
    "DEFAULT_SOURCE_DB_PATH = r\"D:\\college\\imp-doc\\sem6\\GENAI\\project\\3D-Reconstruction-of-Monuments\\chroma1_db\" # Assuming this is the source text DB\n",
    "DEFAULT_ARCH_QA_DB_PATH = r\"D:\\college\\imp-doc\\sem6\\GENAI\\project\\3D-Reconstruction-of-Monuments\\chroma_architecture_qa_db\"\n",
    "DEFAULT_QA_TXT_FILE = r\"C:\\Users\\Rishi S Etagi\\Downloads\\Hampi Architectural Q&A.txt\"\n",
    "EMBEDDING_MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "LLM_MODEL = \"llama3-8b-8192\"\n",
    "# Ensure you have your Groq API key set as an environment variable or replace the placeholder\n",
    "GROQ_API_KEY = os.environ.get(\"GROQ_API_KEY\", \"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\") # Replace with your key if not using env var\n",
    "\n",
    "# List of architectural topics (from store.ipynb)\n",
    "hampi_architectural_topics = [\n",
    "    \"Vijayanagara architectural style\", \"Temple architecture\", \"Gopurams\",\n",
    "    \"Mandapas (pillared halls)\", \"Pillars (musical pillars, ornate pillars)\",\n",
    "    \"Islamic influences on Vijayanagara architecture\", \"Royal Enclosure structures\",\n",
    "    \"Water structures (stepwells, tanks, aqueducts)\", \"Fortifications and gateways\",\n",
    "    \"Bas-reliefs and carvings\", \"Materials used in construction (granite)\",\n",
    "    \"Comparison with other South Indian styles (Dravidian, Chalukya)\",\n",
    "    \"Specific monument features (e.g., Stone Chariot, Lotus Mahal design)\"\n",
    "]\n",
    "\n",
    "# --- Helper Functions ---\n",
    "\n",
    "def initialize_components(source_db_path=DEFAULT_SOURCE_DB_PATH, arch_qa_db_path=DEFAULT_ARCH_QA_DB_PATH):\n",
    "    \"\"\"Initializes embedding model, LLMs, and vector stores.\"\"\"\n",
    "    print(\"Initializing models and vector stores...\")\n",
    "    embedding_model, llm, question_generator_llm, source_vectorstore, architecture_qa_vectorstore, qa_chain = None, None, None, None, None, None # Initialize to None\n",
    "    try:\n",
    "        print(\"--> Initializing Embedding Model...\")\n",
    "        embedding_model = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL_NAME)\n",
    "        print(\"    Embedding Model Initialized.\")\n",
    "\n",
    "        # Source vector store (for context)\n",
    "        print(f\"--> Attempting to load source vector store from: {os.path.abspath(source_db_path)}\")\n",
    "        if not os.path.exists(source_db_path):\n",
    "            print(f\"    Error: Source DB directory not found: {os.path.abspath(source_db_path)}\")\n",
    "            raise FileNotFoundError(f\"Directory not found: {source_db_path}\")\n",
    "        source_vectorstore = Chroma(persist_directory=source_db_path, embedding_function=embedding_model)\n",
    "        print(\"    Source vector store loaded.\")\n",
    "\n",
    "        # Architecture Q&A vector store (for storing generated pairs)\n",
    "        print(f\"--> Attempting to load/create architecture Q&A vector store at: {os.path.abspath(arch_qa_db_path)}\")\n",
    "        architecture_qa_vectorstore = Chroma(persist_directory=arch_qa_db_path, embedding_function=embedding_model)\n",
    "        print(\"    Architecture Q&A vector store loaded/initialized.\")\n",
    "\n",
    "        # LLMs\n",
    "        print(\"--> Initializing LLMs...\")\n",
    "        # Define the placeholder key used as default\n",
    "        placeholder_key = \"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\"\n",
    "        # Modify the check: Error if key is missing OR if it's exactly the placeholder\n",
    "        if not GROQ_API_KEY or GROQ_API_KEY == placeholder_key:\n",
    "             print(\"    Error: GROQ_API_KEY is missing or is the default placeholder.\")\n",
    "             raise ValueError(\"Invalid or missing Groq API Key provided. Ensure it's set via environment variable.\")\n",
    "        llm = ChatGroq(model=LLM_MODEL, groq_api_key=GROQ_API_KEY)\n",
    "        question_generator_llm = ChatGroq(model=LLM_MODEL, groq_api_key=GROQ_API_KEY) # Separate instance if needed\n",
    "        print(\"    LLMs initialized.\")\n",
    "\n",
    "        # RAG chain for answering questions based on source documents\n",
    "        print(\"--> Initializing RAG chain...\")\n",
    "        qa_chain = RetrievalQA.from_chain_type(\n",
    "            llm=llm,\n",
    "            retriever=source_vectorstore.as_retriever()\n",
    "        )\n",
    "        print(\"    RAG chain initialized.\")\n",
    "\n",
    "        print(\"--> Initialization complete (try block finished).\")\n",
    "        # Explicitly return the initialized components here\n",
    "        return embedding_model, llm, question_generator_llm, source_vectorstore, architecture_qa_vectorstore, qa_chain\n",
    "\n",
    "    except Exception as e:\n",
    "        # Print the specific error that occurred during initialization\n",
    "        print(f\"\\n!!! Error during initialization: {e} !!!\\n\")\n",
    "        # Print the full traceback to see where the error originated\n",
    "        traceback.print_exc()\n",
    "        print(\"\\n!!! Please ensure ChromaDB directories exist and are valid, required models are accessible, and API keys are valid. !!!\\n\")\n",
    "        # Return None for all components to signal failure\n",
    "        return None, None, None, None, None, None\n",
    "\n",
    "def generate_architectural_question(question_generator_llm, topic=None):\n",
    "    \"\"\"Generates a specific architectural question about Hampi.\"\"\"\n",
    "    if not question_generator_llm:\n",
    "        return \"Error: Question generator LLM not initialized.\"\n",
    "\n",
    "    # Use random.choice if topic is not provided\n",
    "    selected_topic = topic if topic else random.choice(hampi_architectural_topics)\n",
    "\n",
    "    prompt = f\"\"\"Generate a specific, detailed question about the architectural features of Hampi, focusing on the topic: '{selected_topic}'.\n",
    "    Examples:\n",
    "    - What are the typical dimensions and decorative motifs found on the pillars of the Vitthala Temple's main mandapa?\n",
    "    - Describe the construction techniques used for the corbelled arches seen in the Lotus Mahal.\n",
    "    - How does the design of the gopuram at the Virupaksha Temple incorporate elements from earlier Dravidian styles?\n",
    "\n",
    "    Focus specifically on construction methods, design elements, structural innovations, or artistic aspects of the architecture.\n",
    "    The question should concentrate on architectural style only, not history or cultural significance.\n",
    "    Provide ONLY the question with no additional text.\"\"\"\n",
    "\n",
    "    messages = [\n",
    "        SystemMessage(content=\"You are an architectural historian specializing in Hampi and Vijayanagara architecture.\"),\n",
    "        HumanMessage(content=prompt)\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        response = question_generator_llm.invoke(messages)\n",
    "        # Basic cleaning: remove potential quotes or prefixes\n",
    "        question = response.content.strip().strip('\"').strip(\"'\")\n",
    "        if not question.endswith(\"?\"):\n",
    "             question += \"?\" # Ensure it's a question\n",
    "        return question\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating question: {e}\")\n",
    "        return f\"Could you detail the architectural features of {selected_topic} in Hampi?\" # Fallback\n",
    "\n",
    "def save_qa_to_txt(qa_pairs, filename=DEFAULT_QA_TXT_FILE):\n",
    "    \"\"\"Saves generated Q&A pairs to a text file.\"\"\"\n",
    "    output_dir = os.path.dirname(filename)\n",
    "    if output_dir and not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    try:\n",
    "        with open(filename, \"w\", encoding=\"utf-8\") as file:\n",
    "            file.write(\"Hampi Architectural Q&A\\n\")\n",
    "            file.write(\"=\" * 40 + \"\\n\\n\")\n",
    "\n",
    "            for i, qa in enumerate(qa_pairs):\n",
    "                file.write(f\"Q{i+1}: {qa['question']}\\n\")\n",
    "                file.write(f\"A{i+1}: {qa['answer']}\\n\")\n",
    "                file.write(\"-\" * 40 + \"\\n\\n\")\n",
    "\n",
    "        print(f\"Q&A pairs saved to {filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving Q&A to file {filename}: {e}\")\n",
    "\n",
    "\n",
    "# --- Main Functions for incorporated.py ---\n",
    "\n",
    "def process_architectural_qa_batch_and_save(num_questions=10, arch_qa_db_path=DEFAULT_ARCH_QA_DB_PATH, qa_txt_file=DEFAULT_QA_TXT_FILE, topic=None):\n",
    "    \"\"\"Generates and stores architecture-focused Q&A pairs.\"\"\"\n",
    "    embedding_model, llm, question_generator_llm, source_vectorstore, architecture_qa_vectorstore, qa_chain = initialize_components(arch_qa_db_path=arch_qa_db_path)\n",
    "\n",
    "    # --- Existing Debugging ---\n",
    "    print(\"\\n--- Debugging component values after initialization ---\")\n",
    "    print(f\"  embedding_model: {type(embedding_model)}, Is None: {embedding_model is None}\")\n",
    "    print(f\"  llm: {type(llm)}, Is None: {llm is None}\")\n",
    "    print(f\"  question_generator_llm: {type(question_generator_llm)}, Is None: {question_generator_llm is None}\")\n",
    "    print(f\"  source_vectorstore: {type(source_vectorstore)}, Is None: {source_vectorstore is None}\")\n",
    "    print(f\"  architecture_qa_vectorstore: {type(architecture_qa_vectorstore)}, Is None: {architecture_qa_vectorstore is None}\")\n",
    "    print(f\"  qa_chain: {type(qa_chain)}, Is None: {qa_chain is None}\")\n",
    "    # Keep the all() print for comparison, but don't use it for the check\n",
    "    print(f\"  Result of all([...]): {all([embedding_model, llm, question_generator_llm, source_vectorstore, architecture_qa_vectorstore, qa_chain])}\")\n",
    "    print(\"--- End Debugging ---\\n\")\n",
    "    # --- End Existing Debugging ---\n",
    "\n",
    "    # --- Modified Check: Check each component individually ---\n",
    "    initialization_failed = False\n",
    "    if embedding_model is None:\n",
    "        print(\"Initialization Check Failed: embedding_model is None\")\n",
    "        initialization_failed = True\n",
    "    if llm is None:\n",
    "        print(\"Initialization Check Failed: llm is None\")\n",
    "        initialization_failed = True\n",
    "    if question_generator_llm is None:\n",
    "        print(\"Initialization Check Failed: question_generator_llm is None\")\n",
    "        initialization_failed = True\n",
    "    if source_vectorstore is None:\n",
    "        print(\"Initialization Check Failed: source_vectorstore is None\")\n",
    "        initialization_failed = True\n",
    "    if architecture_qa_vectorstore is None:\n",
    "        print(\"Initialization Check Failed: architecture_qa_vectorstore is None\")\n",
    "        initialization_failed = True\n",
    "    if qa_chain is None:\n",
    "        print(\"Initialization Check Failed: qa_chain is None\")\n",
    "        initialization_failed = True\n",
    "\n",
    "    if initialization_failed:\n",
    "        print(\"Aborting Q&A generation due to initialization failure (individual check).\")\n",
    "        return\n",
    "    # --- End Modified Check ---\n",
    "\n",
    "    # Original check (commented out)\n",
    "    # if not all([embedding_model, llm, question_generator_llm, source_vectorstore, architecture_qa_vectorstore, qa_chain]):\n",
    "    #     print(\"Aborting Q&A generation due to initialization failure.\")\n",
    "    #     return\n",
    "\n",
    "    print(f\"Generating {num_questions} synthetic architecture-focused Q&A pairs about Hampi...\")\n",
    "    qa_pairs = []\n",
    "\n",
    "    for i in range(num_questions):\n",
    "        print(f\"\\n--- Generating Pair {i+1}/{num_questions} ---\")\n",
    "        # 1. Generate Question\n",
    "        question = generate_architectural_question(question_generator_llm, topic=topic)\n",
    "        print(f\"Generated Question: {question}\")\n",
    "\n",
    "        if \"Error:\" in question:\n",
    "            continue # Skip if question generation failed\n",
    "\n",
    "        # 2. Generate Answer using RAG\n",
    "        try:\n",
    "            print(\"Generating answer using RAG...\")\n",
    "            # Use invoke instead of deprecated run\n",
    "            answer_result = qa_chain.invoke({\"query\": question})\n",
    "            answer = answer_result.get(\"result\", \"Could not retrieve an answer.\") # Adjust based on actual output structure\n",
    "            print(f\"Generated Answer: {answer[:150]}...\")\n",
    "\n",
    "            qa_pairs.append({\"question\": question, \"answer\": answer})\n",
    "\n",
    "            # 3. Store in Architecture Vector Store\n",
    "            qa_text = f\"Question: {question}\\nAnswer: {answer}\"\n",
    "            topic_keywords = [t for t in hampi_architectural_topics if any(word in question.lower() for word in t.lower().split())]\n",
    "            metadata_topic = topic_keywords[0] if topic_keywords else \"General Hampi architecture\"\n",
    "\n",
    "            architecture_qa_vectorstore.add_texts(\n",
    "                [qa_text],\n",
    "                metadatas=[{\n",
    "                    \"question\": question,\n",
    "                    \"topic\": metadata_topic,\n",
    "                    \"content_type\": \"architecture\"\n",
    "                }],\n",
    "                ids=[str(uuid.uuid4())] # Ensure unique IDs\n",
    "            )\n",
    "            print(f\"Stored Q&A pair in {arch_qa_db_path}\")\n",
    "\n",
    "            # Sleep briefly to avoid potential API rate limits\n",
    "            time.sleep(1.5)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing Q&A pair for question '{question}': {e}\")\n",
    "            time.sleep(1) # Wait a bit longer after an error\n",
    "\n",
    "    # 4. Save results to TXT file\n",
    "    save_qa_to_txt(qa_pairs, filename=qa_txt_file)\n",
    "\n",
    "    # 5. Persist the vector store changes\n",
    "    try:\n",
    "        print(\"Persisting architecture Q&A vector store...\")\n",
    "        architecture_qa_vectorstore.persist() # Ensure data is saved\n",
    "        print(f\"Completed architectural Q&A generation. Stored {len(qa_pairs)} pairs in {arch_qa_db_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error persisting vector store {arch_qa_db_path}: {e}\")\n",
    "\n",
    "\n",
    "def search_architecture_qa_database(query, k=3, arch_qa_db_path=DEFAULT_ARCH_QA_DB_PATH):\n",
    "    \"\"\"Searches the architecture-specific Q&A vector store.\"\"\"\n",
    "    try:\n",
    "        print(f\"Searching architecture Q&A database at {arch_qa_db_path} for: '{query}'\")\n",
    "        embedding_model = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL_NAME)\n",
    "        vector_store = Chroma(persist_directory=arch_qa_db_path, embedding_function=embedding_model)\n",
    "        results = vector_store.similarity_search(query, k=k)\n",
    "        print(f\"Found {len(results)} results.\")\n",
    "        return results\n",
    "    except Exception as e:\n",
    "        print(f\"Error searching vector store {arch_qa_db_path}: {e}\")\n",
    "        return []\n",
    "\n",
    "# Example of how to run directly (optional)\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Running store_utils.py directly for testing...\")\n",
    "    # Test Q&A generation\n",
    "    # process_architectural_qa_batch_and_save(num_questions=2, qa_txt_file=\"test_hampi_qa.txt\", arch_qa_db_path=\"./test_chroma_arch_qa\")\n",
    "\n",
    "    # Test search (assuming the DB was created and populated)\n",
    "    # test_query = \"Tell me about the pillars in Hampi\"\n",
    "    # search_results = search_architecture_qa_database(test_query, arch_qa_db_path=\"./test_chroma_arch_qa\")\n",
    "    # if search_results:\n",
    "    #     for doc in search_results:\n",
    "    #         print(\"\\n-- Result --\")\n",
    "    #         print(doc.page_content)\n",
    "    # else:\n",
    "    #     print(\"No results found for test query.\")\n",
    "    pass\n",
    "\n",
    "# --- Add this line for debugging ---\n",
    "# print(f\"DEBUG: GROQ_API_KEY from environment: {os.environ.get('GROQ_API_KEY')}\")\n",
    "# --- End of added line ---\n",
    "\n",
    "# Existing line where ChatGroq is initialized (This line seems misplaced outside the function, consider removing it if it's redundant)\n",
    "# llm = ChatGroq(model_name=\"mixtral-8x7b-32768\", groq_api_key=os.environ.get(\"GROQ_API_KEY\")) # Or similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\college\\imp-doc\\sem6\\GENAI\\project\\.venv\\lib\\site-packages\\controlnet_aux\\mediapipe_face\\mediapipe_face_common.py:7: UserWarning: The module 'mediapipe' is not installed. The package will have limited functionality. Please install it using the command: pip install 'mediapipe'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running view_generation.py directly for testing...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler\n",
    "from controlnet_aux import HEDdetector, MLSDdetector, PidiNetDetector, NormalBaeDetector, LineartDetector, LineartAnimeDetector, CannyDetector, ContentShuffleDetector, ZoeDetector, OpenposeDetector\n",
    "from diffusers.utils import load_image\n",
    "from huggingface_hub import hf_hub_download \n",
    "\n",
    "# --- Configuration ---\n",
    "# Choose the appropriate ControlNet model based on desired view generation method\n",
    "# Depth seems suitable for generating different views from a single image\n",
    "CONTROLNET_MODEL_ID = \"lllyasviel/control_v11f1p_sd15_depth\"\n",
    "STABLE_DIFFUSION_MODEL_ID = \"runwayml/stable-diffusion-v1-5\" # Base model for ControlNet\n",
    "\n",
    "# --- Helper Functions ---\n",
    "\n",
    "def load_controlnet_pipeline(device='cuda'):\n",
    "    \"\"\"Loads the ControlNet pipeline for depth-controlled image generation.\"\"\"\n",
    "    print(\"Loading ControlNet pipeline...\")\n",
    "    try:\n",
    "        controlnet = ControlNetModel.from_pretrained(CONTROLNET_MODEL_ID, torch_dtype=torch.float16)\n",
    "        pipe = StableDiffusionControlNetPipeline.from_pretrained(\n",
    "            STABLE_DIFFUSION_MODEL_ID, controlnet=controlnet, torch_dtype=torch.float16\n",
    "        )\n",
    "        pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\n",
    "        # Remove following line if using CPU or have enough VRAM\n",
    "        # pipe.enable_model_cpu_offload() # Offload parts to CPU if VRAM is limited\n",
    "        pipe.to(device)\n",
    "        pipe.enable_xformers_memory_efficient_attention() # Use if xformers is installed\n",
    "        print(\"ControlNet pipeline loaded.\")\n",
    "        return pipe\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading ControlNet pipeline: {e}\")\n",
    "        print(\"Ensure you have the necessary libraries installed and model IDs are correct.\")\n",
    "        return None\n",
    "\n",
    "def get_depth_map(image, device='cuda'):\n",
    "    \"\"\"Generates a depth map for the input image.\"\"\"\n",
    "    print(\"Generating depth map...\")\n",
    "    try:\n",
    "        # Initialize the MiDaS depth estimator\n",
    "        depth_estimator = MiDaSDetector.from_pretrained(\"Intel/dpt-hybrid-midas\")\n",
    "        depth_map_image = depth_estimator(image, detect_resolution=384, image_resolution=512) # Adjust resolutions as needed\n",
    "        print(\"Depth map generated.\")\n",
    "        return depth_map_image\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating depth map: {e}\")\n",
    "        return None\n",
    "\n",
    "# --- Main Function for incorporated.py ---\n",
    "\n",
    "def generate_multiple_views(input_image_path, num_views=4, output_dir=\"hampi_output/views\", device='cuda', prompt_prefix=\"Another view of the Hampi monument\"):\n",
    "    \"\"\"Generates multiple views of an object from a single input image using ControlNet.\"\"\"\n",
    "    if not os.path.exists(input_image_path):\n",
    "        print(f\"Error: Input image not found at {input_image_path}\")\n",
    "        return []\n",
    "\n",
    "    # Load models\n",
    "    controlnet_pipe = load_controlnet_pipeline(device=device)\n",
    "    if not controlnet_pipe:\n",
    "        return []\n",
    "\n",
    "    # Prepare output directory\n",
    "    view_output_dir = os.path.join(output_dir, os.path.splitext(os.path.basename(input_image_path))[0] + \"_views\")\n",
    "    os.makedirs(view_output_dir, exist_ok=True)\n",
    "    print(f\"Saving generated views to: {view_output_dir}\")\n",
    "\n",
    "    # Load input image\n",
    "    try:\n",
    "        input_image = Image.open(input_image_path).convert(\"RGB\")\n",
    "        # Optional: Resize image if needed for consistency or performance\n",
    "        # input_image = input_image.resize((512, 512))\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading input image {input_image_path}: {e}\")\n",
    "        return []\n",
    "\n",
    "    # Get depth map (Control Image)\n",
    "    control_image = get_depth_map(input_image, device=device)\n",
    "    if not control_image:\n",
    "        return []\n",
    "\n",
    "    # Define base prompt and negative prompt\n",
    "    # You might want to extract details from the filename or use a fixed prompt\n",
    "    base_prompt = f\"{prompt_prefix}, realistic photo, ancient stone architecture, Hampi, India, detailed stonework, clear daylight, high resolution\"\n",
    "    negative_prompt = \"cartoon, illustration, anime, 3d render, painting, sketch, drawing, blur, distortion, low quality, poor lighting, oversaturated, fantasy elements, text, words, signature, watermark\"\n",
    "\n",
    "    generated_image_paths = []\n",
    "\n",
    "    print(f\"Generating {num_views} different views...\")\n",
    "    for i in tqdm(range(num_views)):\n",
    "        # Generate image with ControlNet\n",
    "        # You can slightly vary the prompt or seed for different views\n",
    "        # For more distinct views, you might need more sophisticated techniques\n",
    "        # like manipulating the depth map or using different ControlNets (e.g., Normal maps, Canny edges)\n",
    "        # or dedicated multi-view models if available.\n",
    "        # Here, we rely on the inherent randomness of the diffusion process with a fixed control.\n",
    "        try:\n",
    "            generator = torch.Generator(device=device).manual_seed(i * 1234 + 5678) # Vary seed for variation\n",
    "            output_image = controlnet_pipe(\n",
    "                prompt=base_prompt,\n",
    "                negative_prompt=negative_prompt,\n",
    "                image=control_image, # Provide the depth map as the control\n",
    "                num_inference_steps=30, # Adjust steps (20-50 typical)\n",
    "                guidance_scale=7.5,     # Adjust guidance\n",
    "                generator=generator\n",
    "            ).images[0]\n",
    "\n",
    "            # Save the generated image\n",
    "            filename = f\"view_{i+1}.png\"\n",
    "            save_path = os.path.join(view_output_dir, filename)\n",
    "            output_image.save(save_path)\n",
    "            generated_image_paths.append(save_path)\n",
    "            print(f\"Saved view {i+1} to {save_path}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error generating view {i+1}: {e}\")\n",
    "\n",
    "    print(f\"Generated {len(generated_image_paths)} views.\")\n",
    "    return generated_image_paths\n",
    "\n",
    "\n",
    "# Example usage (optional)\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Running view_generation.py directly for testing...\")\n",
    "    # Create a dummy input image file for testing if needed\n",
    "    # dummy_image_path = \"dummy_input.png\"\n",
    "    # if not os.path.exists(dummy_image_path):\n",
    "    #     Image.new('RGB', (512, 512), color = 'red').save(dummy_image_path)\n",
    "\n",
    "    # test_input_image = dummy_image_path # Replace with a real image path\n",
    "    # if os.path.exists(test_input_image):\n",
    "    #     generate_multiple_views(\n",
    "    #         input_image_path=test_input_image,\n",
    "    #         num_views=2,\n",
    "    #         output_dir=\"test_hampi_output/views\",\n",
    "    #         device='cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    #     )\n",
    "    # else:\n",
    "    #     print(f\"Test input image '{test_input_image}' not found.\")\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "from datetime import datetime\n",
    "import torch\n",
    "import re\n",
    "\n",
    "\n",
    "DEFAULT_REFERENCE_IMAGES_DIR = \"D:\\\\college\\\\imp-doc\\\\sem6\\\\GENAI\\\\project\\\\new\\\\chariot\\\\images\"\n",
    "\n",
    "# try:\n",
    "#     from store_utils import process_architectural_qa_batch_and_save, search_architecture_qa_database, DEFAULT_QA_TXT_FILE, DEFAULT_ARCH_QA_DB_PATH\n",
    "#     print(\"- Successfully imported: Q&A Generation & Search Utilities\")\n",
    "# except ImportError as e:\n",
    "#     print(f\"Warning: Could not import from store_utils.py: {e}\")\n",
    "#     process_architectural_qa_batch_and_save = None\n",
    "#     search_architecture_qa_database = None\n",
    "#     # Define defaults here if import fails, to avoid NameError later\n",
    "#     DEFAULT_QA_TXT_FILE = r\"C:\\Users\\Rishi S Etagi\\Downloads\\Hampi Architectural Q&A.txt\"\n",
    "#     DEFAULT_ARCH_QA_DB_PATH = r\"C:\\Users\\Rishi S Etagi\\Desktop\\gen ai\\bruh\\3D-Reconstruction-of-Monuments\\chroma_architecture_qa_db\"\n",
    "\n",
    "\n",
    "# try:\n",
    "#     from view_generation import generate_multiple_views\n",
    "#     print(\"- Successfully imported: 3D View Generation Utilities\")\n",
    "# except ImportError as e:\n",
    "#     print(f\"Warning: Could not import from view_generation.py: {e}\")\n",
    "#     generate_multiple_views = None\n",
    "\n",
    "# print(\"Imports complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Define default parameters for the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Default paths and settings\n",
    "REFERENCE_DIR = DEFAULT_REFERENCE_IMAGES_DIR\n",
    "OUTPUT_DIR = \"hampi_output\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "\n",
    "# Device configuration\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Operation Modes\n",
    "\n",
    "Here we define different operation modes for the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Operation mode: \"generate_qa\", \"search_qa\", \"generate_images\", \"generate_views\", \"full_pipeline\"\n",
    "def run_generate_qa_mode(num_questions=10, qa_topic=None, output_file=DEFAULT_QA_TXT_FILE, qa_db_path=DEFAULT_ARCH_QA_DB_PATH):\n",
    "    \"\"\"Generate Q&A pairs about Hampi architecture\"\"\"\n",
    "    if process_architectural_qa_batch_and_save:\n",
    "        print(f\"Generating {num_questions} architectural Q&A pairs...\")\n",
    "        qa_output_filepath = os.path.join(OUTPUT_DIR, os.path.basename(output_file))\n",
    "        process_architectural_qa_batch_and_save(\n",
    "            num_questions=num_questions,\n",
    "            arch_qa_db_path=qa_db_path,\n",
    "            qa_txt_file=qa_output_filepath,\n",
    "            topic=qa_topic\n",
    "        )\n",
    "        print(f\"Q&A pairs saved to {qa_output_filepath}\")\n",
    "        return qa_output_filepath\n",
    "    else:\n",
    "        print(\"Error: Q&A generation function not available. Please check imports and store_utils.py.\")\n",
    "        return None\n",
    "\n",
    "def run_search_qa_mode(query, qa_db_path=DEFAULT_ARCH_QA_DB_PATH):\n",
    "    \"\"\"Search the Q&A database for architectural information\"\"\"\n",
    "    if search_architecture_qa_database and query:\n",
    "        print(f\"Searching architectural Q&A database for: '{query}'\")\n",
    "        results = search_architecture_qa_database(\n",
    "            query=query,\n",
    "            arch_qa_db_path=qa_db_path\n",
    "        )\n",
    "        print(\"\\nSearch Results:\")\n",
    "        if results:\n",
    "            for i, doc in enumerate(results):\n",
    "                print(f\"\\n--- Result {i+1} ---\")\n",
    "                # Extract Q&A from the stored text\n",
    "                content = doc.page_content\n",
    "                q_match = re.search(r'Question: (.*?)(?=\\nAnswer:|$)', content)\n",
    "                a_match = re.search(r'Answer: (.*?)(?=$)', content, re.DOTALL)\n",
    "                \n",
    "                if q_match: print(f\"Q: {q_match.group(1).strip()}\")\n",
    "                if a_match: print(f\"A: {a_match.group(1).strip()}\")\n",
    "                print(f\"(Similarity Score: {doc.metadata.get('_distance', 'N/A')})\")\n",
    "                print(\"----------\")\n",
    "        else:\n",
    "            print(\"No relevant results found in the architectural Q&A database.\")\n",
    "        return results\n",
    "    elif not query:\n",
    "         print(\"Error: Please provide a query for search_qa mode.\")\n",
    "    else:\n",
    "        print(\"Error: Q&A search function not available. Please check imports and store_utils.py.\")\n",
    "    return None\n",
    "\n",
    "def run_generate_images_mode(source_type=\"file\", input_file=DEFAULT_QA_TXT_FILE, \n",
    "                          qa_db_path=DEFAULT_ARCH_QA_DB_PATH, query=None,\n",
    "                          num_images=2, monuments=None,\n",
    "                          reference_dir=REFERENCE_DIR, reference_count=5):\n",
    "    \"\"\"Generate images from Q&A data using reference images\"\"\"\n",
    "    if qa_to_images_pipeline:\n",
    "        print(\"Starting image generation pipeline...\")\n",
    "        # Construct full path for input file if using file source\n",
    "        qa_input_filepath = os.path.join(OUTPUT_DIR, os.path.basename(input_file)) if source_type == 'file' else input_file\n",
    "\n",
    "        # Pass reference image directory and count to the pipeline\n",
    "        generated_image_paths = qa_to_images_pipeline(\n",
    "            source_type=source_type,\n",
    "            file_path=qa_input_filepath,\n",
    "            vector_store_path=qa_db_path,\n",
    "            query=query,\n",
    "            num_images=num_images,\n",
    "            output_dir=OUTPUT_DIR,\n",
    "            monument_filters=monuments,\n",
    "            reference_dir=reference_dir,\n",
    "            reference_count=reference_count,\n",
    "            device=DEVICE\n",
    "        )\n",
    "        return generated_image_paths\n",
    "    else:\n",
    "        print(\"Error: Image generation function not available. Please check imports and qa_to_image_pipeline.py.\")\n",
    "        return []\n",
    "\n",
    "def run_generate_views_mode(input_image, num_views=4):\n",
    "    \"\"\"Generate multiple views of an input image\"\"\"\n",
    "    if generate_multiple_views and input_image:\n",
    "        print(f\"Generating {num_views} views for image: {input_image}\")\n",
    "        if not os.path.exists(input_image):\n",
    "             print(f\"Error: Input image not found at {input_image}\")\n",
    "             return []\n",
    "        else:\n",
    "            # Views will be saved in a subdirectory within output_dir by the function\n",
    "            view_paths = generate_multiple_views(\n",
    "                input_image_path=input_image,\n",
    "                num_views=num_views,\n",
    "                output_dir=OUTPUT_DIR,\n",
    "                device=DEVICE\n",
    "            )\n",
    "            return view_paths\n",
    "    elif not input_image:\n",
    "         print(\"Error: Please provide an input image for generate_views mode.\")\n",
    "    else:\n",
    "        print(\"Error: 3D view generation function not available. Please check imports and view_generation.py.\")\n",
    "    return []\n",
    "\n",
    "def run_full_pipeline(num_questions=10, qa_topic=None, num_images=2, monuments=None, \n",
    "                     num_views=4, reference_dir=REFERENCE_DIR, reference_count=5):\n",
    "    \"\"\"Run the complete pipeline: generate Q&A, images, and views\"\"\"\n",
    "    start_time = datetime.now()\n",
    "    print(\"Running full pipeline...\")\n",
    "    qa_file_for_images = os.path.join(OUTPUT_DIR, os.path.basename(DEFAULT_QA_TXT_FILE))\n",
    "\n",
    "    # Step 1: Generate Q&A\n",
    "    if process_architectural_qa_batch_and_save:\n",
    "        print(f\"\\n--- Step 1: Generating {num_questions} Q&A pairs ---\")\n",
    "        qa_file_path = run_generate_qa_mode(\n",
    "            num_questions=num_questions,\n",
    "            qa_topic=qa_topic,\n",
    "            output_file=DEFAULT_QA_TXT_FILE\n",
    "        )\n",
    "        print(\"--- Q&A Generation Complete ---\")\n",
    "        if not qa_file_path:\n",
    "            print(\"Error in Q&A generation, cannot proceed with full pipeline\")\n",
    "            return None\n",
    "    else:\n",
    "        print(\"\\nWarning: Q&A generation function not available. Checking for existing file...\")\n",
    "        # Check if the specified input file exists if we skip generation\n",
    "        if not os.path.exists(qa_file_for_images):\n",
    "             print(f\"Error: Q&A input file '{qa_file_for_images}' not found and generation was skipped. Cannot proceed.\")\n",
    "             return None\n",
    "        print(f\"Found existing file: {qa_file_for_images}\")\n",
    "\n",
    "    # Step 2: Generate Images from Q&A\n",
    "    if qa_to_images_pipeline:\n",
    "        print(f\"\\n--- Step 2: Generating {num_images} images per Q&A pair ---\")\n",
    "        generated_image_paths = run_generate_images_mode(\n",
    "            source_type=\"file\",\n",
    "            input_file=DEFAULT_QA_TXT_FILE,\n",
    "            num_images=num_images,\n",
    "            monuments=monuments,\n",
    "            reference_dir=reference_dir,\n",
    "            reference_count=reference_count\n",
    "        )\n",
    "        print(\"--- Image Generation Complete ---\")\n",
    "    else:\n",
    "        print(\"\\nWarning: Image generation function not available. Skipping Step 2.\")\n",
    "        generated_image_paths = []\n",
    "\n",
    "    # Step 3: Generate Views for a random generated image\n",
    "    if generate_multiple_views and generated_image_paths:\n",
    "        # Select one of the generated images randomly\n",
    "        image_to_process = random.choice(generated_image_paths)\n",
    "        print(f\"\\n--- Step 3: Generating {num_views} views for a generated image: {os.path.basename(image_to_process)} ---\")\n",
    "        view_paths = run_generate_views_mode(\n",
    "            input_image=image_to_process,\n",
    "            num_views=num_views\n",
    "        )\n",
    "        print(\"--- View Generation Complete ---\")\n",
    "    elif not generated_image_paths:\n",
    "         print(\"\\nSkipping view generation (Step 3) as no images were generated in Step 2.\")\n",
    "    else:\n",
    "        print(\"\\nWarning: 3D view generation function not available. Skipping Step 3.\")\n",
    "\n",
    "    end_time = datetime.now()\n",
    "    print(f\"\\n{'='*15} Pipeline Finished {'='*15}\")\n",
    "    print(f\"Total execution time: {end_time - start_time}\")\n",
    "    print(f\"{'='*50}\\n\")\n",
    "    \n",
    "    return generated_image_paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Usage\n",
    "\n",
    "Below are examples of how to use each mode of the pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Generate Q&A Pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 5 architectural Q&A pairs...\n",
      "Initializing models and vector stores...\n",
      "--> Initializing Embedding Model...\n",
      "\n",
      "!!! Error during initialization: Failed to import transformers.models.timm_wrapper.configuration_timm_wrapper because of the following error (look up to see its traceback):\n",
      "cannot import name 'ImageNetInfo' from 'timm.data' (d:\\college\\imp-doc\\sem6\\GENAI\\project\\.venv\\lib\\site-packages\\timm\\data\\__init__.py) !!!\n",
      "\n",
      "\n",
      "!!! Please ensure ChromaDB directories exist and are valid, required models are accessible, and API keys are valid. !!!\n",
      "\n",
      "\n",
      "--- Debugging component values after initialization ---\n",
      "  embedding_model: <class 'NoneType'>, Is None: True\n",
      "  llm: <class 'NoneType'>, Is None: True\n",
      "  question_generator_llm: <class 'NoneType'>, Is None: True\n",
      "  source_vectorstore: <class 'NoneType'>, Is None: True\n",
      "  architecture_qa_vectorstore: <class 'NoneType'>, Is None: True\n",
      "  qa_chain: <class 'NoneType'>, Is None: True\n",
      "  Result of all([...]): False\n",
      "--- End Debugging ---\n",
      "\n",
      "Initialization Check Failed: embedding_model is None\n",
      "Initialization Check Failed: llm is None\n",
      "Initialization Check Failed: question_generator_llm is None\n",
      "Initialization Check Failed: source_vectorstore is None\n",
      "Initialization Check Failed: architecture_qa_vectorstore is None\n",
      "Initialization Check Failed: qa_chain is None\n",
      "Aborting Q&A generation due to initialization failure (individual check).\n",
      "Q&A pairs saved to hampi_output\\Hampi Architectural Q&A.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"d:\\college\\imp-doc\\sem6\\GENAI\\project\\.venv\\lib\\site-packages\\transformers\\utils\\import_utils.py\", line 1967, in _get_module\n",
      "    return importlib.import_module(\".\" + module_name, self.__name__)\n",
      "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.9_3.9.3568.0_x64__qbz5n2kfra8p0\\lib\\importlib\\__init__.py\", line 127, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"<frozen importlib._bootstrap>\", line 1030, in _gcd_import\n",
      "  File \"<frozen importlib._bootstrap>\", line 1007, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 986, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 680, in _load_unlocked\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 850, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 228, in _call_with_frames_removed\n",
      "  File \"d:\\college\\imp-doc\\sem6\\GENAI\\project\\.venv\\lib\\site-packages\\transformers\\models\\timm_wrapper\\configuration_timm_wrapper.py\", line 25, in <module>\n",
      "    from timm.data import ImageNetInfo, infer_imagenet_subset\n",
      "ImportError: cannot import name 'ImageNetInfo' from 'timm.data' (d:\\college\\imp-doc\\sem6\\GENAI\\project\\.venv\\lib\\site-packages\\timm\\data\\__init__.py)\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\sameer\\AppData\\Local\\Temp\\ipykernel_31712\\726527592.py\", line 43, in initialize_components\n",
      "    embedding_model = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL_NAME)\n",
      "  File \"d:\\college\\imp-doc\\sem6\\GENAI\\project\\.venv\\lib\\site-packages\\langchain_huggingface\\embeddings\\huggingface.py\", line 59, in __init__\n",
      "    self._client = sentence_transformers.SentenceTransformer(\n",
      "  File \"d:\\college\\imp-doc\\sem6\\GENAI\\project\\.venv\\lib\\site-packages\\sentence_transformers\\SentenceTransformer.py\", line 287, in __init__\n",
      "    modules = self._load_sbert_model(\n",
      "  File \"d:\\college\\imp-doc\\sem6\\GENAI\\project\\.venv\\lib\\site-packages\\sentence_transformers\\SentenceTransformer.py\", line 1487, in _load_sbert_model\n",
      "    module = Transformer(model_name_or_path, cache_dir=cache_folder, **kwargs)\n",
      "  File \"d:\\college\\imp-doc\\sem6\\GENAI\\project\\.venv\\lib\\site-packages\\sentence_transformers\\models\\Transformer.py\", line 54, in __init__\n",
      "    self._load_model(model_name_or_path, config, cache_dir, **model_args)\n",
      "  File \"d:\\college\\imp-doc\\sem6\\GENAI\\project\\.venv\\lib\\site-packages\\sentence_transformers\\models\\Transformer.py\", line 85, in _load_model\n",
      "    self.auto_model = AutoModel.from_pretrained(\n",
      "  File \"d:\\college\\imp-doc\\sem6\\GENAI\\project\\.venv\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py\", line 548, in from_pretrained\n",
      "    has_local_code = type(config) in cls._model_mapping.keys()\n",
      "  File \"d:\\college\\imp-doc\\sem6\\GENAI\\project\\.venv\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py\", line 787, in keys\n",
      "    mapping_keys = [\n",
      "  File \"d:\\college\\imp-doc\\sem6\\GENAI\\project\\.venv\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py\", line 788, in <listcomp>\n",
      "    self._load_attr_from_module(key, name)\n",
      "  File \"d:\\college\\imp-doc\\sem6\\GENAI\\project\\.venv\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py\", line 784, in _load_attr_from_module\n",
      "    return getattribute_from_module(self._modules[module_name], attr)\n",
      "  File \"d:\\college\\imp-doc\\sem6\\GENAI\\project\\.venv\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py\", line 700, in getattribute_from_module\n",
      "    if hasattr(module, attr):\n",
      "  File \"d:\\college\\imp-doc\\sem6\\GENAI\\project\\.venv\\lib\\site-packages\\transformers\\utils\\import_utils.py\", line 1955, in __getattr__\n",
      "    module = self._get_module(self._class_to_module[name])\n",
      "  File \"d:\\college\\imp-doc\\sem6\\GENAI\\project\\.venv\\lib\\site-packages\\transformers\\utils\\import_utils.py\", line 1969, in _get_module\n",
      "    raise RuntimeError(\n",
      "RuntimeError: Failed to import transformers.models.timm_wrapper.configuration_timm_wrapper because of the following error (look up to see its traceback):\n",
      "cannot import name 'ImageNetInfo' from 'timm.data' (d:\\college\\imp-doc\\sem6\\GENAI\\project\\.venv\\lib\\site-packages\\timm\\data\\__init__.py)\n"
     ]
    }
   ],
   "source": [
    "# Generate 5 Q&A pairs about Hampi architecture\n",
    "qa_file_path = run_generate_qa_mode(num_questions=5, qa_topic=\"Stone chariot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Generate Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting image generation pipeline...\n",
      "Loading models...\n",
      "Loading models...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Cannot initialize model with low cpu memory usage because `accelerate` was not found in the environment. Defaulting to `low_cpu_mem_usage=False`. It is strongly recommended to install `accelerate` for faster and less memory-intense model loading. You can do so with: \n",
      "```\n",
      "pip install accelerate\n",
      "```\n",
      ".\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading SD model: SG161222/Realistic_Vision_V5.1_noVAE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading pipeline components...: 100%|██████████| 5/5 [00:03<00:00,  1.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading img2img model...\n",
      "Models loaded successfully\n",
      "Retrieving Q&A pairs...\n",
      "Found 16 Q&A pairs.\n",
      "Filtered to 1 Q&A pairs related to specified monuments.\n",
      "\n",
      "Processing Q&A pair 1/1\n",
      "Question: How does the Stone Chariot at Hampi demonstrate the advanced engineering and architectural skills of the Vijayanagara craftsmen, particularly in terms of creating the illusion of a monolithic structure while actually using multiple granite blocks?\n",
      "Extracted details: The Stone Chariot at Hampi stands as a remarkable testament to the engineering prowess of Vijayanaga...\n",
      "Generated prompt: A monument at Hampi, India, The Stone Chariot at Hampi stands as a remarkable testament to the engin...\n",
      "Loading reference images from D:\\college\\imp-doc\\sem6\\GENAI\\project\\new\\chariot\\images...\n",
      "Found 1 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading images: 100%|██████████| 1/1 [00:00<00:00, 87.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded 1 reference images\n",
      "Using reference image: 2000.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 34/34 [00:08<00:00,  3.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved image to hampi_output\\hampi_1_seed640907784.png\n",
      "Saved comparison to hampi_output\\hampi_1_comparison.png\n",
      "\n",
      "Generation complete. Created 1 images in hampi_output.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from PIL import Image as PILImage\n",
    "img = PILImage.open(DEFAULT_REFERENCE_IMAGES_DIR + \"\\\\2000.png\")\n",
    "img.show()\n",
    "# Generate images from Q&A pairs\n",
    "image_paths = run_generate_images_mode(\n",
    "    source_type=\"file\",\n",
    "    input_file=r\"D:\\college\\imp-doc\\sem6\\GENAI\\project\\3D-Reconstruction-of-Monuments\\hampi_output\\Hampi_Architecture_QA.txt\",\n",
    "    num_images=1,\n",
    "    monuments=[\"Stone Chariot\"],\n",
    "    reference_dir=REFERENCE_DIR,\n",
    "    reference_count=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Generate Views of Existing Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate multiple views of an existing image\n",
    "# If you already have generated images, provide the path to one of them\n",
    "# existing_image = \"hampi_output/vittala_temple_0.png\"  # Replace with actual path\n",
    "# views = run_generate_views_mode(input_image=existing_image, num_views=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Run Full Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the complete pipeline\n",
    "# results = run_full_pipeline(\n",
    "#     num_questions=10,\n",
    "#     qa_topic=\"Hampi architectural features\",\n",
    "#     num_images=2,\n",
    "#     monuments=[\"Vittala Temple\"],\n",
    "#     num_views=3,\n",
    "#     reference_dir=REFERENCE_DIR,\n",
    "#     reference_count=5\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display Generated Images\n",
    "\n",
    "You can use the following cell to display generated images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display a generated image\n",
    "from IPython.display import Image, display\n",
    "\n",
    "def display_images(image_paths, max_images=5):\n",
    "    \"\"\"Display images in the notebook\"\"\"\n",
    "    if not image_paths:\n",
    "        print(\"No images to display\")\n",
    "        return\n",
    "    \n",
    "    for i, path in enumerate(image_paths[:max_images]):\n",
    "        if os.path.exists(path):\n",
    "            print(f\"Image {i+1}: {os.path.basename(path)}\")\n",
    "            display(Image(path, width=800))\n",
    "        else:\n",
    "            print(f\"Image file not found: {path}\")\n",
    "    \n",
    "    if len(image_paths) > max_images:\n",
    "        print(f\"... and {len(image_paths) - max_images} more images\")\n",
    "\n",
    "# Uncomment to display images from a previous run\n",
    "# image_dir = \"hampi_output\"\n",
    "# all_images = [os.path.join(image_dir, f) for f in os.listdir(image_dir) if f.endswith(\".png\") and not \"comparison\" in f]\n",
    "# display_images(all_images)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
