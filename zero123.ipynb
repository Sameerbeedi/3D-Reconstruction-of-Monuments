{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2a71475",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3044c859",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2.5.1+cu121'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cdfd395e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install diffusers==0.20.2 transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ff41397",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install safetensors ftfy scipy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ffe725ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in d:\\college\\imp-doc\\sem6\\genai\\project\\.venv\\lib\\site-packages (4.51.2)\n",
      "Requirement already satisfied: filelock in d:\\college\\imp-doc\\sem6\\genai\\project\\.venv\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Collecting huggingface-hub<1.0,>=0.30.0 (from transformers)\n",
      "  Using cached huggingface_hub-0.30.2-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in d:\\college\\imp-doc\\sem6\\genai\\project\\.venv\\lib\\site-packages (from transformers) (1.26.3)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\college\\imp-doc\\sem6\\genai\\project\\.venv\\lib\\site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in d:\\college\\imp-doc\\sem6\\genai\\project\\.venv\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in d:\\college\\imp-doc\\sem6\\genai\\project\\.venv\\lib\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in d:\\college\\imp-doc\\sem6\\genai\\project\\.venv\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in d:\\college\\imp-doc\\sem6\\genai\\project\\.venv\\lib\\site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in d:\\college\\imp-doc\\sem6\\genai\\project\\.venv\\lib\\site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in d:\\college\\imp-doc\\sem6\\genai\\project\\.venv\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in d:\\college\\imp-doc\\sem6\\genai\\project\\.venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in d:\\college\\imp-doc\\sem6\\genai\\project\\.venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n",
      "Requirement already satisfied: colorama in d:\\college\\imp-doc\\sem6\\genai\\project\\.venv\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\college\\imp-doc\\sem6\\genai\\project\\.venv\\lib\\site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\college\\imp-doc\\sem6\\genai\\project\\.venv\\lib\\site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\college\\imp-doc\\sem6\\genai\\project\\.venv\\lib\\site-packages (from requests->transformers) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\college\\imp-doc\\sem6\\genai\\project\\.venv\\lib\\site-packages (from requests->transformers) (2025.1.31)\n",
      "Using cached huggingface_hub-0.30.2-py3-none-any.whl (481 kB)\n",
      "Installing collected packages: huggingface-hub\n",
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface-hub 0.25.2\n",
      "    Uninstalling huggingface-hub-0.25.2:\n",
      "      Successfully uninstalled huggingface-hub-0.25.2\n",
      "Successfully installed huggingface-hub-0.30.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\college\\imp-doc\\sem6\\GENAI\\project\\.venv\\lib\\site-packages\\huggingface_hub\\file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  headers: Dict[str, str],\n",
      "vae\\diffusion_pytorch_model.safetensors not found\n",
      "Fetching 16 files: 100%|██████████| 16/16 [01:44<00:00,  6.53s/it]\n",
      "Cannot initialize model with low cpu memory usage because `accelerate` was not found in the environment. Defaulting to `low_cpu_mem_usage=False`. It is strongly recommended to install `accelerate` for faster and less memory-intense model loading. You can do so with: \n",
      "```\n",
      "pip install accelerate\n",
      "```\n",
      ".\n",
      "Loading pipeline components...:  25%|██▌       | 2/8 [00:00<00:01,  3.03it/s]d:\\college\\imp-doc\\sem6\\GENAI\\project\\.venv\\lib\\site-packages\\diffusers\\models\\modeling_utils.py:106: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(checkpoint_file, map_location=\"cpu\")\n",
      "Loading pipeline components...: 100%|██████████| 8/8 [00:19<00:00,  2.39s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Zero123PlusPipeline {\n",
       "  \"_class_name\": \"Zero123PlusPipeline\",\n",
       "  \"_diffusers_version\": \"0.20.2\",\n",
       "  \"_name_or_path\": \"sudo-ai/zero123plus-v1.1\",\n",
       "  \"feature_extractor_clip\": [\n",
       "    \"transformers\",\n",
       "    \"CLIPImageProcessor\"\n",
       "  ],\n",
       "  \"feature_extractor_vae\": [\n",
       "    \"transformers\",\n",
       "    \"CLIPImageProcessor\"\n",
       "  ],\n",
       "  \"ramping_coefficients\": [\n",
       "    0.0,\n",
       "    0.2060057818889618,\n",
       "    0.18684479594230652,\n",
       "    0.24342191219329834,\n",
       "    0.18507817387580872,\n",
       "    0.1703828126192093,\n",
       "    0.15628913044929504,\n",
       "    0.14174538850784302,\n",
       "    0.13617539405822754,\n",
       "    0.13569170236587524,\n",
       "    0.1269884556531906,\n",
       "    0.1200924888253212,\n",
       "    0.12816639244556427,\n",
       "    0.13058121502399445,\n",
       "    0.14201879501342773,\n",
       "    0.15004529058933258,\n",
       "    0.1620427817106247,\n",
       "    0.17207716405391693,\n",
       "    0.18534132838249207,\n",
       "    0.20002241432666779,\n",
       "    0.21657466888427734,\n",
       "    0.22996725142002106,\n",
       "    0.24613411724567413,\n",
       "    0.25141021609306335,\n",
       "    0.26613450050354004,\n",
       "    0.271847128868103,\n",
       "    0.2850190997123718,\n",
       "    0.285749226808548,\n",
       "    0.2813953757286072,\n",
       "    0.29509517550468445,\n",
       "    0.30109965801239014,\n",
       "    0.31370124220848083,\n",
       "    0.3134534955024719,\n",
       "    0.3108579218387604,\n",
       "    0.32147032022476196,\n",
       "    0.33548328280448914,\n",
       "    0.3301997184753418,\n",
       "    0.3254660964012146,\n",
       "    0.3514464199542999,\n",
       "    0.35993096232414246,\n",
       "    0.3510829508304596,\n",
       "    0.37661612033843994,\n",
       "    0.3913513123989105,\n",
       "    0.42122599482536316,\n",
       "    0.3954688012599945,\n",
       "    0.4260983467102051,\n",
       "    0.479139506816864,\n",
       "    0.4588979482650757,\n",
       "    0.4873477816581726,\n",
       "    0.5095643401145935,\n",
       "    0.5133851170539856,\n",
       "    0.520708441734314,\n",
       "    0.5363377928733826,\n",
       "    0.5661528706550598,\n",
       "    0.5859065651893616,\n",
       "    0.6207258701324463,\n",
       "    0.6560986638069153,\n",
       "    0.6379964351654053,\n",
       "    0.6777164340019226,\n",
       "    0.6589891910552979,\n",
       "    0.7574057579040527,\n",
       "    0.7446827292442322,\n",
       "    0.7695522308349609,\n",
       "    0.8163619041442871,\n",
       "    0.9502472281455994,\n",
       "    0.9918442368507385,\n",
       "    0.9398387670516968,\n",
       "    1.005432367324829,\n",
       "    0.9295969605445862,\n",
       "    0.9899859428405762,\n",
       "    1.044832706451416,\n",
       "    1.0427014827728271,\n",
       "    1.0829696655273438,\n",
       "    1.0062562227249146,\n",
       "    1.0966323614120483,\n",
       "    1.0550328493118286,\n",
       "    1.2108079195022583\n",
       "  ],\n",
       "  \"safety_checker\": [\n",
       "    null,\n",
       "    null\n",
       "  ],\n",
       "  \"scheduler\": [\n",
       "    \"diffusers\",\n",
       "    \"EulerAncestralDiscreteScheduler\"\n",
       "  ],\n",
       "  \"text_encoder\": [\n",
       "    \"transformers\",\n",
       "    \"CLIPTextModel\"\n",
       "  ],\n",
       "  \"tokenizer\": [\n",
       "    \"transformers\",\n",
       "    \"CLIPTokenizer\"\n",
       "  ],\n",
       "  \"unet\": [\n",
       "    \"diffusers\",\n",
       "    \"UNet2DConditionModel\"\n",
       "  ],\n",
       "  \"vae\": [\n",
       "    \"diffusers\",\n",
       "    \"AutoencoderKL\"\n",
       "  ],\n",
       "  \"vision_encoder\": [\n",
       "    \"transformers\",\n",
       "    \"CLIPVisionModelWithProjection\"\n",
       "  ]\n",
       "}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# !pip uninstall huggingface_hub -y\n",
    "# !pip install huggingface_hub==0.25.2 -i https://pypi.tuna.tsinghua.edu.cn/simple\n",
    "!pip install transformers -U\n",
    "import torch\n",
    "import requests\n",
    "from PIL import Image\n",
    "from huggingface_hub import hf_hub_download\n",
    "from diffusers import DiffusionPipeline, EulerAncestralDiscreteScheduler\n",
    "\n",
    "# Load the pipeline\n",
    "pipeline = DiffusionPipeline.from_pretrained(\n",
    "    \"sudo-ai/zero123plus-v1.1\", custom_pipeline=\"sudo-ai/zero123plus-pipeline\",\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "\n",
    "# Feel free to tune the scheduler!\n",
    "# `timestep_spacing` parameter is not supported in older versions of `diffusers`\n",
    "# so there may be performance degradations\n",
    "# We recommend using `diffusers==0.20.2`\n",
    "pipeline.scheduler = EulerAncestralDiscreteScheduler.from_config(\n",
    "    pipeline.scheduler.config, timestep_spacing='trailing'\n",
    ")\n",
    "pipeline.to('cuda:0')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1e3abff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [01:07<00:00,  1.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image saved as output_20250412_161014.png\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from PIL import Image\n",
    "\n",
    "# Download an example image.\n",
    "cond = Image.open(\"D:\\\\college\\\\imp-doc\\\\sem6\\\\GENAI\\\\project\\\\3D-Reconstruction-of-Monuments\\\\1002-removebg-preview.png\")\n",
    "\n",
    "# Run the pipeline!\n",
    "result = pipeline(cond, num_inference_steps=100).images[0]\n",
    "\n",
    "# Display the result\n",
    "result.show()\n",
    "\n",
    "# Generate a unique filename using the current timestamp\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "output_filename = f\"output_{timestamp}.png\"\n",
    "\n",
    "# Save the result with the unique filename\n",
    "result.save(output_filename)\n",
    "\n",
    "print(f\"Image saved as {output_filename}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
