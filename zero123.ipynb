{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a71475",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: torch 2.1.0\n",
      "Uninstalling torch-2.1.0:\n",
      "  Successfully uninstalled torch-2.1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Failed to remove contents in a temporary directory 'D:\\college\\imp-doc\\sem6\\GENAI\\project\\.venv\\Lib\\site-packages\\~orch'.\n",
      "You can safely remove it manually.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cu124\n",
      "Collecting torch\n",
      "  Using cached https://download.pytorch.org/whl/cu124/torch-2.6.0%2Bcu124-cp39-cp39-win_amd64.whl.metadata (28 kB)\n",
      "Requirement already satisfied: torchvision in d:\\college\\imp-doc\\sem6\\genai\\project\\.venv\\lib\\site-packages (0.16.0)\n",
      "Requirement already satisfied: torchaudio in d:\\college\\imp-doc\\sem6\\genai\\project\\.venv\\lib\\site-packages (2.1.0)\n",
      "Requirement already satisfied: filelock in d:\\college\\imp-doc\\sem6\\genai\\project\\.venv\\lib\\site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in d:\\college\\imp-doc\\sem6\\genai\\project\\.venv\\lib\\site-packages (from torch) (4.13.2)\n",
      "Requirement already satisfied: networkx in d:\\college\\imp-doc\\sem6\\genai\\project\\.venv\\lib\\site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in d:\\college\\imp-doc\\sem6\\genai\\project\\.venv\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in d:\\college\\imp-doc\\sem6\\genai\\project\\.venv\\lib\\site-packages (from torch) (2023.6.0)\n",
      "Collecting sympy==1.13.1 (from torch)\n",
      "  Using cached https://download.pytorch.org/whl/sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in d:\\college\\imp-doc\\sem6\\genai\\project\\.venv\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: numpy in d:\\college\\imp-doc\\sem6\\genai\\project\\.venv\\lib\\site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: requests in d:\\college\\imp-doc\\sem6\\genai\\project\\.venv\\lib\\site-packages (from torchvision) (2.32.3)\n",
      "INFO: pip is looking at multiple versions of torchvision to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting torchvision\n",
      "  Using cached https://download.pytorch.org/whl/cu124/torchvision-0.21.0%2Bcu124-cp39-cp39-win_amd64.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in d:\\college\\imp-doc\\sem6\\genai\\project\\.venv\\lib\\site-packages (from torchvision) (9.5.0)\n",
      "INFO: pip is looking at multiple versions of torchaudio to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting torchaudio\n",
      "  Using cached https://download.pytorch.org/whl/cu124/torchaudio-2.6.0%2Bcu124-cp39-cp39-win_amd64.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\college\\imp-doc\\sem6\\genai\\project\\.venv\\lib\\site-packages (from jinja2->torch) (2.1.5)\n",
      "Using cached https://download.pytorch.org/whl/cu124/torch-2.6.0%2Bcu124-cp39-cp39-win_amd64.whl (2532.2 MB)\n",
      "Using cached https://download.pytorch.org/whl/cu124/torchvision-0.21.0%2Bcu124-cp39-cp39-win_amd64.whl (6.1 MB)\n",
      "Using cached https://download.pytorch.org/whl/cu124/torchaudio-2.6.0%2Bcu124-cp39-cp39-win_amd64.whl (4.2 MB)\n",
      "Installing collected packages: sympy, torch, torchvision, torchaudio\n",
      "  Attempting uninstall: sympy\n",
      "    Found existing installation: sympy 1.13.3\n",
      "    Uninstalling sympy-1.13.3:\n",
      "      Successfully uninstalled sympy-1.13.3\n",
      "  Attempting uninstall: torchvision\n",
      "    Found existing installation: torchvision 0.16.0\n",
      "    Uninstalling torchvision-0.16.0:\n",
      "      Successfully uninstalled torchvision-0.16.0\n",
      "  Attempting uninstall: torchaudio\n",
      "    Found existing installation: torchaudio 2.1.0\n",
      "    Uninstalling torchaudio-2.1.0:\n",
      "      Successfully uninstalled torchaudio-2.1.0\n",
      "Successfully installed sympy-1.13.1 torch-2.6.0+cu124 torchaudio-2.6.0+cu124 torchvision-0.21.0+cu124\n"
     ]
    }
   ],
   "source": [
    "# !pip uninstall torch -y\n",
    "# !pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3044c859",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2.6.0+cu124'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cdfd395e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install diffusers==0.20.2 transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5ff41397",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: diffusers in d:\\college\\imp-doc\\sem6\\genai\\project\\.venv\\lib\\site-packages (0.20.2)\n",
      "Collecting diffusers\n",
      "  Using cached diffusers-0.33.1-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: importlib-metadata in d:\\college\\imp-doc\\sem6\\genai\\project\\.venv\\lib\\site-packages (from diffusers) (8.6.1)\n",
      "Requirement already satisfied: filelock in d:\\college\\imp-doc\\sem6\\genai\\project\\.venv\\lib\\site-packages (from diffusers) (3.13.1)\n",
      "Collecting huggingface-hub>=0.27.0 (from diffusers)\n",
      "  Using cached huggingface_hub-0.30.2-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy in d:\\college\\imp-doc\\sem6\\genai\\project\\.venv\\lib\\site-packages (from diffusers) (1.26.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in d:\\college\\imp-doc\\sem6\\genai\\project\\.venv\\lib\\site-packages (from diffusers) (2024.11.6)\n",
      "Requirement already satisfied: requests in d:\\college\\imp-doc\\sem6\\genai\\project\\.venv\\lib\\site-packages (from diffusers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in d:\\college\\imp-doc\\sem6\\genai\\project\\.venv\\lib\\site-packages (from diffusers) (0.5.3)\n",
      "Requirement already satisfied: Pillow in d:\\college\\imp-doc\\sem6\\genai\\project\\.venv\\lib\\site-packages (from diffusers) (11.0.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in d:\\college\\imp-doc\\sem6\\genai\\project\\.venv\\lib\\site-packages (from huggingface-hub>=0.27.0->diffusers) (2024.6.1)\n",
      "Requirement already satisfied: packaging>=20.9 in d:\\college\\imp-doc\\sem6\\genai\\project\\.venv\\lib\\site-packages (from huggingface-hub>=0.27.0->diffusers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in d:\\college\\imp-doc\\sem6\\genai\\project\\.venv\\lib\\site-packages (from huggingface-hub>=0.27.0->diffusers) (6.0.2)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in d:\\college\\imp-doc\\sem6\\genai\\project\\.venv\\lib\\site-packages (from huggingface-hub>=0.27.0->diffusers) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in d:\\college\\imp-doc\\sem6\\genai\\project\\.venv\\lib\\site-packages (from huggingface-hub>=0.27.0->diffusers) (4.13.2)\n",
      "Requirement already satisfied: zipp>=3.20 in d:\\college\\imp-doc\\sem6\\genai\\project\\.venv\\lib\\site-packages (from importlib-metadata->diffusers) (3.21.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\college\\imp-doc\\sem6\\genai\\project\\.venv\\lib\\site-packages (from requests->diffusers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\college\\imp-doc\\sem6\\genai\\project\\.venv\\lib\\site-packages (from requests->diffusers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\college\\imp-doc\\sem6\\genai\\project\\.venv\\lib\\site-packages (from requests->diffusers) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\college\\imp-doc\\sem6\\genai\\project\\.venv\\lib\\site-packages (from requests->diffusers) (2025.1.31)\n",
      "Requirement already satisfied: colorama in d:\\college\\imp-doc\\sem6\\genai\\project\\.venv\\lib\\site-packages (from tqdm>=4.42.1->huggingface-hub>=0.27.0->diffusers) (0.4.6)\n",
      "Using cached diffusers-0.33.1-py3-none-any.whl (3.6 MB)\n",
      "Using cached huggingface_hub-0.30.2-py3-none-any.whl (481 kB)\n",
      "Installing collected packages: huggingface-hub, diffusers\n",
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface-hub 0.15.1\n",
      "    Uninstalling huggingface-hub-0.15.1:\n",
      "      Successfully uninstalled huggingface-hub-0.15.1\n",
      "  Attempting uninstall: diffusers\n",
      "    Found existing installation: diffusers 0.20.2\n",
      "    Uninstalling diffusers-0.20.2:\n",
      "      Successfully uninstalled diffusers-0.20.2\n",
      "Successfully installed diffusers-0.33.1 huggingface-hub-0.30.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "langchain-huggingface 0.1.2 requires tokenizers>=0.19.1, but you have tokenizers 0.13.3 which is incompatible.\n",
      "langchain-huggingface 0.1.2 requires transformers>=4.39.0, but you have transformers 4.30.0 which is incompatible.\n",
      "sentence-transformers 4.1.0 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.30.0 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "# !pip uninstall diffusers -y\n",
    "# !pip uninstall transformers -y\n",
    "# !pip uninstall huggingface_hub -y\n",
    "# !pip install diffusers==0.20.2\n",
    "# !pip install transformers==4.30.0\n",
    "# !pip install huggingface_hub==0.15.1\n",
    "!pip install diffusers --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ffe725ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\college\\imp-doc\\sem6\\GENAI\\project\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Cannot initialize model with low cpu memory usage because `accelerate` was not found in the environment. Defaulting to `low_cpu_mem_usage=False`. It is strongly recommended to install `accelerate` for faster and less memory-intense model loading. You can do so with: \n",
      "```\n",
      "pip install accelerate\n",
      "```\n",
      ".\n",
      "Loading pipeline components...:  62%|██████▎   | 5/8 [00:01<00:01,  2.87it/s]An error occurred while trying to fetch C:\\Users\\sameer\\.cache\\huggingface\\hub\\models--sudo-ai--zero123plus-v1.1\\snapshots\\36df7de980afd15f80b2e1a4e9a920d7020e2654\\vae: Error no file named diffusion_pytorch_model.safetensors found in directory C:\\Users\\sameer\\.cache\\huggingface\\hub\\models--sudo-ai--zero123plus-v1.1\\snapshots\\36df7de980afd15f80b2e1a4e9a920d7020e2654\\vae.\n",
      "Defaulting to unsafe serialization. Pass `allow_pickle=False` to raise an error instead.\n",
      "Loading pipeline components...:  88%|████████▊ | 7/8 [00:01<00:00,  4.59it/s]An error occurred while trying to fetch C:\\Users\\sameer\\.cache\\huggingface\\hub\\models--sudo-ai--zero123plus-v1.1\\snapshots\\36df7de980afd15f80b2e1a4e9a920d7020e2654\\unet: Error no file named diffusion_pytorch_model.safetensors found in directory C:\\Users\\sameer\\.cache\\huggingface\\hub\\models--sudo-ai--zero123plus-v1.1\\snapshots\\36df7de980afd15f80b2e1a4e9a920d7020e2654\\unet.\n",
      "Defaulting to unsafe serialization. Pass `allow_pickle=False` to raise an error instead.\n",
      "Loading pipeline components...: 100%|██████████| 8/8 [00:02<00:00,  3.98it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Zero123PlusPipeline {\n",
       "  \"_class_name\": \"Zero123PlusPipeline\",\n",
       "  \"_diffusers_version\": \"0.33.1\",\n",
       "  \"_name_or_path\": \"sudo-ai/zero123plus-v1.1\",\n",
       "  \"feature_extractor_clip\": [\n",
       "    \"transformers\",\n",
       "    \"CLIPImageProcessor\"\n",
       "  ],\n",
       "  \"feature_extractor_vae\": [\n",
       "    \"transformers\",\n",
       "    \"CLIPImageProcessor\"\n",
       "  ],\n",
       "  \"ramping_coefficients\": [\n",
       "    0.0,\n",
       "    0.2060057818889618,\n",
       "    0.18684479594230652,\n",
       "    0.24342191219329834,\n",
       "    0.18507817387580872,\n",
       "    0.1703828126192093,\n",
       "    0.15628913044929504,\n",
       "    0.14174538850784302,\n",
       "    0.13617539405822754,\n",
       "    0.13569170236587524,\n",
       "    0.1269884556531906,\n",
       "    0.1200924888253212,\n",
       "    0.12816639244556427,\n",
       "    0.13058121502399445,\n",
       "    0.14201879501342773,\n",
       "    0.15004529058933258,\n",
       "    0.1620427817106247,\n",
       "    0.17207716405391693,\n",
       "    0.18534132838249207,\n",
       "    0.20002241432666779,\n",
       "    0.21657466888427734,\n",
       "    0.22996725142002106,\n",
       "    0.24613411724567413,\n",
       "    0.25141021609306335,\n",
       "    0.26613450050354004,\n",
       "    0.271847128868103,\n",
       "    0.2850190997123718,\n",
       "    0.285749226808548,\n",
       "    0.2813953757286072,\n",
       "    0.29509517550468445,\n",
       "    0.30109965801239014,\n",
       "    0.31370124220848083,\n",
       "    0.3134534955024719,\n",
       "    0.3108579218387604,\n",
       "    0.32147032022476196,\n",
       "    0.33548328280448914,\n",
       "    0.3301997184753418,\n",
       "    0.3254660964012146,\n",
       "    0.3514464199542999,\n",
       "    0.35993096232414246,\n",
       "    0.3510829508304596,\n",
       "    0.37661612033843994,\n",
       "    0.3913513123989105,\n",
       "    0.42122599482536316,\n",
       "    0.3954688012599945,\n",
       "    0.4260983467102051,\n",
       "    0.479139506816864,\n",
       "    0.4588979482650757,\n",
       "    0.4873477816581726,\n",
       "    0.5095643401145935,\n",
       "    0.5133851170539856,\n",
       "    0.520708441734314,\n",
       "    0.5363377928733826,\n",
       "    0.5661528706550598,\n",
       "    0.5859065651893616,\n",
       "    0.6207258701324463,\n",
       "    0.6560986638069153,\n",
       "    0.6379964351654053,\n",
       "    0.6777164340019226,\n",
       "    0.6589891910552979,\n",
       "    0.7574057579040527,\n",
       "    0.7446827292442322,\n",
       "    0.7695522308349609,\n",
       "    0.8163619041442871,\n",
       "    0.9502472281455994,\n",
       "    0.9918442368507385,\n",
       "    0.9398387670516968,\n",
       "    1.005432367324829,\n",
       "    0.9295969605445862,\n",
       "    0.9899859428405762,\n",
       "    1.044832706451416,\n",
       "    1.0427014827728271,\n",
       "    1.0829696655273438,\n",
       "    1.0062562227249146,\n",
       "    1.0966323614120483,\n",
       "    1.0550328493118286,\n",
       "    1.2108079195022583\n",
       "  ],\n",
       "  \"safety_checker\": [\n",
       "    null,\n",
       "    null\n",
       "  ],\n",
       "  \"scheduler\": [\n",
       "    \"diffusers\",\n",
       "    \"EulerAncestralDiscreteScheduler\"\n",
       "  ],\n",
       "  \"text_encoder\": [\n",
       "    \"transformers\",\n",
       "    \"CLIPTextModel\"\n",
       "  ],\n",
       "  \"tokenizer\": [\n",
       "    \"transformers\",\n",
       "    \"CLIPTokenizer\"\n",
       "  ],\n",
       "  \"unet\": [\n",
       "    \"diffusers\",\n",
       "    \"UNet2DConditionModel\"\n",
       "  ],\n",
       "  \"vae\": [\n",
       "    \"diffusers\",\n",
       "    \"AutoencoderKL\"\n",
       "  ],\n",
       "  \"vision_encoder\": [\n",
       "    \"transformers\",\n",
       "    \"CLIPVisionModelWithProjection\"\n",
       "  ]\n",
       "}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# !pip uninstall diffusers transformers huggingface_hub -y\n",
    "# !pip install diffusers==0.20.2 transformers==4.30.0 huggingface_hub==0.15.1\n",
    "# !pip install --upgrade diffusers\n",
    "# !pip install --upgrade transformers\n",
    "# !pip install --upgrade huggingface_hub\n",
    "\n",
    "import torch\n",
    "import requests\n",
    "from PIL import Image\n",
    "from huggingface_hub import hf_hub_download\n",
    "from diffusers import DiffusionPipeline, EulerAncestralDiscreteScheduler\n",
    "\n",
    "# Load the pipeline\n",
    "pipeline = DiffusionPipeline.from_pretrained(\n",
    "    \"sudo-ai/zero123plus-v1.1\", custom_pipeline=\"sudo-ai/zero123plus-pipeline\",\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "\n",
    "# Feel free to tune the scheduler!\n",
    "# `timestep_spacing` parameter is not supported in older versions of `diffusers`\n",
    "# so there may be performance degradations\n",
    "# We recommend using `diffusers==0.20.2`\n",
    "pipeline.scheduler = EulerAncestralDiscreteScheduler.from_config(\n",
    "    pipeline.scheduler.config, timestep_spacing='trailing'\n",
    ")\n",
    "pipeline.to('cuda:0')\n",
    "\n",
    "#First, let's find where the custom pipeline is downloaded\n",
    "# import os\n",
    "# from huggingface_hub import hf_hub_download\n",
    "\n",
    "# pipeline_path = hf_hub_download(repo_id=\"sudo-ai/zero123plus-pipeline\", filename=\"pipeline.py\")\n",
    "# print(f\"Custom pipeline is at: {pipeline_path}\")\n",
    "\n",
    "# # You can then examine the file to see if it uses cached_download\n",
    "# with open(pipeline_path, 'r') as f:\n",
    "#     content = f.read()\n",
    "#     if 'cached_download' in content:\n",
    "#         print(\"Found cached_download in the pipeline file!\")\n",
    "\n",
    "# import torch\n",
    "# import diffusers\n",
    "# from diffusers import DiffusionPipeline\n",
    "\n",
    "# # Load without using custom pipeline first\n",
    "# pipeline = DiffusionPipeline.from_pretrained(\n",
    "#     \"sudo-ai/zero123plus-v1.1\",\n",
    "#     torch_dtype=torch.float16\n",
    "# )\n",
    "# pipeline = pipeline.to(\"cuda\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e3abff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:34<00:00,  2.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image 1 saved as context\\output_20250423_211956_0.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:35<00:00,  2.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image 2 saved as context\\output_20250423_212035_1.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 10/100 [00:03<00:32,  2.80it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 19\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Generate and save images in the same directory\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_images):\n\u001b[1;32m---> 19\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcond\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_inference_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mimages[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;66;03m# Optional: show the result\u001b[39;00m\n\u001b[0;32m     22\u001b[0m     result\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[1;32md:\\college\\imp-doc\\sem6\\GENAI\\project\\.venv\\lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\.cache\\huggingface\\modules\\diffusers_modules\\local\\sudo-ai--zero123plus-pipeline\\983e66d28a3637ddd8e3e2fd8165cdff32230872\\pipeline.py:383\u001b[0m, in \u001b[0;36mZero123PlusPipeline.__call__\u001b[1;34m(self, image, prompt, num_images_per_prompt, guidance_scale, depth_image, output_type, width, height, num_inference_steps, return_dict, *args, **kwargs)\u001b[0m\n\u001b[0;32m    381\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munet, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontrolnet\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    382\u001b[0m     cak[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontrol_depth\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m depth_image\n\u001b[1;32m--> 383\u001b[0m latents: torch\u001b[38;5;241m.\u001b[39mTensor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(\n\u001b[0;32m    384\u001b[0m     \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    385\u001b[0m     \u001b[38;5;241m*\u001b[39margs,\n\u001b[0;32m    386\u001b[0m     cross_attention_kwargs\u001b[38;5;241m=\u001b[39mcak,\n\u001b[0;32m    387\u001b[0m     guidance_scale\u001b[38;5;241m=\u001b[39mguidance_scale,\n\u001b[0;32m    388\u001b[0m     num_images_per_prompt\u001b[38;5;241m=\u001b[39mnum_images_per_prompt,\n\u001b[0;32m    389\u001b[0m     prompt_embeds\u001b[38;5;241m=\u001b[39mencoder_hidden_states,\n\u001b[0;32m    390\u001b[0m     num_inference_steps\u001b[38;5;241m=\u001b[39mnum_inference_steps,\n\u001b[0;32m    391\u001b[0m     output_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlatent\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    392\u001b[0m     width\u001b[38;5;241m=\u001b[39mwidth,\n\u001b[0;32m    393\u001b[0m     height\u001b[38;5;241m=\u001b[39mheight,\n\u001b[0;32m    394\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    395\u001b[0m )\u001b[38;5;241m.\u001b[39mimages\n\u001b[0;32m    396\u001b[0m latents \u001b[38;5;241m=\u001b[39m unscale_latents(latents)\n\u001b[0;32m    397\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m output_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlatent\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32md:\\college\\imp-doc\\sem6\\GENAI\\project\\.venv\\lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\college\\imp-doc\\sem6\\GENAI\\project\\.venv\\lib\\site-packages\\diffusers\\pipelines\\stable_diffusion\\pipeline_stable_diffusion.py:1074\u001b[0m, in \u001b[0;36mStableDiffusionPipeline.__call__\u001b[1;34m(self, prompt, height, width, num_inference_steps, timesteps, sigmas, guidance_scale, negative_prompt, num_images_per_prompt, eta, generator, latents, prompt_embeds, negative_prompt_embeds, ip_adapter_image, ip_adapter_image_embeds, output_type, return_dict, cross_attention_kwargs, guidance_rescale, clip_skip, callback_on_step_end, callback_on_step_end_tensor_inputs, **kwargs)\u001b[0m\n\u001b[0;32m   1072\u001b[0m \u001b[38;5;66;03m# call the callback, if provided\u001b[39;00m\n\u001b[0;32m   1073\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(timesteps) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m ((i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m>\u001b[39m num_warmup_steps \u001b[38;5;129;01mand\u001b[39;00m (i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscheduler\u001b[38;5;241m.\u001b[39morder \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m):\n\u001b[1;32m-> 1074\u001b[0m     \u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1075\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m callback \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m i \u001b[38;5;241m%\u001b[39m callback_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1076\u001b[0m         step_idx \u001b[38;5;241m=\u001b[39m i \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscheduler, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124morder\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32md:\\college\\imp-doc\\sem6\\GENAI\\project\\.venv\\lib\\site-packages\\tqdm\\std.py:1242\u001b[0m, in \u001b[0;36mtqdm.update\u001b[1;34m(self, n)\u001b[0m\n\u001b[0;32m   1240\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ema_dn(dn)\n\u001b[0;32m   1241\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ema_dt(dt)\n\u001b[1;32m-> 1242\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrefresh\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlock_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlock_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1243\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdynamic_miniters:\n\u001b[0;32m   1244\u001b[0m     \u001b[38;5;66;03m# If no `miniters` was specified, adjust automatically to the\u001b[39;00m\n\u001b[0;32m   1245\u001b[0m     \u001b[38;5;66;03m# maximum iteration rate seen so far between two prints.\u001b[39;00m\n\u001b[0;32m   1246\u001b[0m     \u001b[38;5;66;03m# e.g.: After running `tqdm.update(5)`, subsequent\u001b[39;00m\n\u001b[0;32m   1247\u001b[0m     \u001b[38;5;66;03m# calls to `tqdm.update()` will only cause an update after\u001b[39;00m\n\u001b[0;32m   1248\u001b[0m     \u001b[38;5;66;03m# at least 5 more iterations.\u001b[39;00m\n\u001b[0;32m   1249\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmaxinterval \u001b[38;5;129;01mand\u001b[39;00m dt \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmaxinterval:\n",
      "File \u001b[1;32md:\\college\\imp-doc\\sem6\\GENAI\\project\\.venv\\lib\\site-packages\\tqdm\\std.py:1347\u001b[0m, in \u001b[0;36mtqdm.refresh\u001b[1;34m(self, nolock, lock_args)\u001b[0m\n\u001b[0;32m   1345\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1346\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39macquire()\n\u001b[1;32m-> 1347\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdisplay\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1348\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m nolock:\n\u001b[0;32m   1349\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32md:\\college\\imp-doc\\sem6\\GENAI\\project\\.venv\\lib\\site-packages\\tqdm\\std.py:1495\u001b[0m, in \u001b[0;36mtqdm.display\u001b[1;34m(self, msg, pos)\u001b[0m\n\u001b[0;32m   1493\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pos:\n\u001b[0;32m   1494\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmoveto(pos)\n\u001b[1;32m-> 1495\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msp\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__str__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pos:\n\u001b[0;32m   1497\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmoveto(\u001b[38;5;241m-\u001b[39mpos)\n",
      "File \u001b[1;32md:\\college\\imp-doc\\sem6\\GENAI\\project\\.venv\\lib\\site-packages\\tqdm\\std.py:459\u001b[0m, in \u001b[0;36mtqdm.status_printer.<locals>.print_status\u001b[1;34m(s)\u001b[0m\n\u001b[0;32m    457\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mprint_status\u001b[39m(s):\n\u001b[0;32m    458\u001b[0m     len_s \u001b[38;5;241m=\u001b[39m disp_len(s)\n\u001b[1;32m--> 459\u001b[0m     \u001b[43mfp_write\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;130;43;01m\\r\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43ms\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mmax\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mlast_len\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlen_s\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    460\u001b[0m     last_len[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m len_s\n",
      "File \u001b[1;32md:\\college\\imp-doc\\sem6\\GENAI\\project\\.venv\\lib\\site-packages\\tqdm\\std.py:453\u001b[0m, in \u001b[0;36mtqdm.status_printer.<locals>.fp_write\u001b[1;34m(s)\u001b[0m\n\u001b[0;32m    451\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfp_write\u001b[39m(s):\n\u001b[0;32m    452\u001b[0m     fp\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;28mstr\u001b[39m(s))\n\u001b[1;32m--> 453\u001b[0m     \u001b[43mfp_flush\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\college\\imp-doc\\sem6\\GENAI\\project\\.venv\\lib\\site-packages\\tqdm\\utils.py:196\u001b[0m, in \u001b[0;36mDisableOnWriteError.disable_on_exception.<locals>.inner\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    194\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    195\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 196\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    197\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    198\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m e\u001b[38;5;241m.\u001b[39merrno \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m5\u001b[39m:\n",
      "File \u001b[1;32md:\\college\\imp-doc\\sem6\\GENAI\\project\\.venv\\lib\\site-packages\\ipykernel\\iostream.py:604\u001b[0m, in \u001b[0;36mOutStream.flush\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    593\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"trigger actual zmq send\u001b[39;00m\n\u001b[0;32m    594\u001b[0m \n\u001b[0;32m    595\u001b[0m \u001b[38;5;124;03msend will happen in the background thread\u001b[39;00m\n\u001b[0;32m    596\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    597\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    598\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpub_thread\n\u001b[0;32m    599\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpub_thread\u001b[38;5;241m.\u001b[39mthread \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    602\u001b[0m ):\n\u001b[0;32m    603\u001b[0m     \u001b[38;5;66;03m# request flush on the background thread\u001b[39;00m\n\u001b[1;32m--> 604\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpub_thread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mschedule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flush\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    605\u001b[0m     \u001b[38;5;66;03m# wait for flush to actually get through, if we can.\u001b[39;00m\n\u001b[0;32m    606\u001b[0m     evt \u001b[38;5;241m=\u001b[39m threading\u001b[38;5;241m.\u001b[39mEvent()\n",
      "File \u001b[1;32md:\\college\\imp-doc\\sem6\\GENAI\\project\\.venv\\lib\\site-packages\\ipykernel\\iostream.py:267\u001b[0m, in \u001b[0;36mIOPubThread.schedule\u001b[1;34m(self, f)\u001b[0m\n\u001b[0;32m    265\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_events\u001b[38;5;241m.\u001b[39mappend(f)\n\u001b[0;32m    266\u001b[0m     \u001b[38;5;66;03m# wake event thread (message content is ignored)\u001b[39;00m\n\u001b[1;32m--> 267\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_event_pipe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    269\u001b[0m     f()\n",
      "File \u001b[1;32md:\\college\\imp-doc\\sem6\\GENAI\\project\\.venv\\lib\\site-packages\\zmq\\sugar\\socket.py:698\u001b[0m, in \u001b[0;36mSocket.send\u001b[1;34m(self, data, flags, copy, track, routing_id, group)\u001b[0m\n\u001b[0;32m    691\u001b[0m         data \u001b[38;5;241m=\u001b[39m zmq\u001b[38;5;241m.\u001b[39mFrame(\n\u001b[0;32m    692\u001b[0m             data,\n\u001b[0;32m    693\u001b[0m             track\u001b[38;5;241m=\u001b[39mtrack,\n\u001b[0;32m    694\u001b[0m             copy\u001b[38;5;241m=\u001b[39mcopy \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    695\u001b[0m             copy_threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy_threshold,\n\u001b[0;32m    696\u001b[0m         )\n\u001b[0;32m    697\u001b[0m     data\u001b[38;5;241m.\u001b[39mgroup \u001b[38;5;241m=\u001b[39m group\n\u001b[1;32m--> 698\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mflags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrack\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrack\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m_zmq.py:1081\u001b[0m, in \u001b[0;36mzmq.backend.cython._zmq.Socket.send\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m_zmq.py:1129\u001b[0m, in \u001b[0;36mzmq.backend.cython._zmq.Socket.send\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m_zmq.py:1397\u001b[0m, in \u001b[0;36mzmq.backend.cython._zmq._send_copy\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m_zmq.py:169\u001b[0m, in \u001b[0;36mzmq.backend.cython._zmq._check_rc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "# Path to the input image\n",
    "input_image_path = \"D:\\\\college\\\\imp-doc\\\\sem6\\\\GENAI\\\\project\\\\3D-Reconstruction-of-Monuments\\\\0-removebg-preview.png\"\n",
    "\n",
    "# Load the image\n",
    "cond = Image.open(input_image_path)\n",
    "\n",
    "# Extract the directory where the input image is stored\n",
    "context_dir = \"context\"\n",
    "\n",
    "# Number of images to generate\n",
    "num_images = 5\n",
    "\n",
    "# Generate and save images in the same directory\n",
    "for i in range(num_images):\n",
    "    result = pipeline(cond, num_inference_steps=100).images[0]\n",
    "    \n",
    "    # Optional: show the result\n",
    "    result.show()\n",
    "\n",
    "    # Create a unique filename\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    output_filename = f\"output_{timestamp}_{i}.png\"\n",
    "    output_path = os.path.join(context_dir, output_filename)\n",
    "\n",
    "    # Save the result\n",
    "    result.save(output_path)\n",
    "    print(f\"Image {i + 1} saved as {output_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
